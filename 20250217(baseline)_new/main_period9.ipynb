{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from methods.model import *\n",
    "from methods.logger import *\n",
    "from methods.processing import *\n",
    "from methods.train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read Factor.\n"
     ]
    }
   ],
   "source": [
    "mp.set_start_method('spawn', force=True)\n",
    "main_device_name = 0\n",
    "print('Read Factor.')\n",
    "factor = pd.read_pickle('/home/datamake117/data/haris/dataset_0121/total_date.pkl')                      # 日期+股票代码\n",
    "grouped = pd.read_pickle('/home/datamake117/data/haris/dataset_0121/grouped_adj.pkl').fillna(0)          # 特征\n",
    "grouped_label = pd.read_pickle('/home/datamake117/data/haris/dataset_0121/grouped_label_adj.pkl')        # 标签\n",
    "grouped_liquidity = pd.read_pickle('/home/datamake117/data/haris/dataset_0121/grouped_liquidity.pkl')    # 流动性指标\n",
    "grouped_liquidity.index = grouped_liquidity.index.strftime('%Y%m%d').astype(int)\n",
    "correlation_df = pd.read_pickle('/home/datamake117/data/haris/dataset_0121/corr_byday_abs_old.pkl')          # 因子筛选辅助数据\n",
    "correlation_df.index = correlation_df.index.strftime('%Y%m%d').astype(int)\n",
    "total_date_list = np.array(factor['date'].drop_duplicates().tolist())                                   # 日期列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    round_num, dt1, dt2, dt3, dt4, dt5,\n",
    "    correlation_df, grouped, grouped_label, grouped_liquidity,\n",
    "    total_date_list, main_folder_name, \n",
    "    pid_num=5269, factor_num=2790, corr_thres=0.9, seed_num=5, model_mode=False, multi_model=6\n",
    "    ):\n",
    "    '''\n",
    "    para round_num: 轮数（周期序号）\n",
    "    para dt1: 训练集开始时间\n",
    "    para dt2: 验证集开始时间\n",
    "    para dt3: 验证集结束时间\n",
    "    para dt4: 测试集开始时间\n",
    "    para dt5: 测试集结束时间\n",
    "    \n",
    "    dt1 ------训练集------ dt2 ------验证集------ dt3/dt4 ------测试集------ dt5\n",
    "    \n",
    "    para correlation_df: 因子筛选辅助数据\n",
    "    para grouped: 按日期分组的因子数据\n",
    "    para grouped_label: 按日期分组的标签数据\n",
    "    para grouped_liquidity: 按日期分组的流动性数据\n",
    "    para total_date_list: 全部日期\n",
    "    para main_folder_name: 主文件夹名称\n",
    "    para pid_num: 股票数量\n",
    "    para factor_num: 因子数量\n",
    "    para corr_thres: 因子筛选相关系数阈值\n",
    "    para seed_num: 每个模型的种子数\n",
    "    para model_mode: 是否继续训练\n",
    "    para multi_model: 模型数量\n",
    "    '''\n",
    "    seed_list = []\n",
    "    for i in range(seed_num):\n",
    "        random.seed(i)\n",
    "        seed_list.append(list(random.sample(range(100), multi_model)))\n",
    "    total_train_num = len(seed_list)  # seed_num * multi_model\n",
    "    total_test_output = []\n",
    "    total_test_name = 'test_output_' + str(round_num) + '.pt'\n",
    "    total_date_pid_name = 'test_date_pid_' + str(round_num) + '.pt'\n",
    "    save_path = \"/home/datamake117/data/haris/DL/\" + main_folder_name\n",
    "    \n",
    "    # 根据给定的时间范围 dt1 到 dt3，选出训练集的日期列表。之后，有一个特别的日期范围处理（过滤掉指定日期段的训练数据）。\n",
    "    date_list_train = total_date_list[np.where((total_date_list >= dt1) & (total_date_list < dt3))[0]]\n",
    "    # 若20240223在训练周期或测试周期内，训练周期或测试周期去除20240201-20240223这一时间段\n",
    "    if 20240223 >= dt1 and 20240223 <= dt3:\n",
    "        date_list_train = np.array([date_train for date_train in date_list_train if date_train < 20240201 or date_train > 20240223])\n",
    "    total_ts_train_val1 = np.zeros((len(date_list_train), pid_num, factor_num)) # 因子数据 shape: (len(date_list_train), pid_num, factor_num)\n",
    "    total_label_train_val = np.zeros((len(date_list_train), pid_num, 5))        # 标签数据 shape: (len(date_list_train), pid_num, 5)\n",
    "    total_group_train_val = np.zeros((len(date_list_train), pid_num, 1))        # 流动性数据 shape: (len(date_list_train), pid_num, 1)\n",
    "    for i in trange(len(date_list_train), desc='train_val_data'):\n",
    "        date = date_list_train[i]\n",
    "        total_ts_train_val1[i, :, :] = grouped.loc[date].iloc[:pid_num, :]          # 因子\n",
    "        total_label_train_val[i, :, :] = grouped_label.loc[date].iloc[:pid_num, :]  # 标签\n",
    "        # 根据流动性调整收益率前7%-10%附近的训练标签：label(returns)\n",
    "        total_label_train_val[i, :, 0] = adjust_daily_returns(total_label_train_val[i, :, 0], total_label_train_val[i, :, 4])\n",
    "        total_group_train_val[i, :, :] = np.array(grouped_liquidity.loc[date])[:pid_num].reshape(-1, 1)  # 流动性\n",
    "    \n",
    "    # 类似地，date_list_test 被定义为测试集的日期范围，时间从 dt4 到 dt5。\n",
    "    date_list_test = total_date_list[np.where((total_date_list >= dt4) & (total_date_list < dt5))[0]]\n",
    "    total_ts_test1 = np.zeros((len(date_list_test), pid_num, factor_num))\n",
    "    total_label_test = np.zeros((len(date_list_test), pid_num, 5))\n",
    "    total_group_test = np.zeros((len(date_list_test), pid_num, 1))\n",
    "    for i in trange(len(date_list_test), desc='test_data'):\n",
    "        date = date_list_test[i]\n",
    "        total_ts_test1[i, :, :] = grouped.loc[date].iloc[:pid_num, :]\n",
    "        total_label_test[i, :, :] = grouped_label.loc[date].iloc[:pid_num, :]\n",
    "        total_label_test[i, :, 0] = adjust_daily_returns(total_label_test[i, :, 0], total_label_test[i, :, 4])\n",
    "        total_group_test[i, :, :] = np.array(grouped_liquidity.loc[date])[:pid_num].reshape(-1, 1)\n",
    "    \n",
    "    # 流动性数据归一化\n",
    "    def min_max_standard(column):\n",
    "        return (column - column.min()) / (column.max() - column.min())\n",
    "    print('Min-max scaling.')\n",
    "    total_group_train_val, total_group_test = min_max_standard(total_group_train_val), min_max_standard(total_group_test)\n",
    "    \n",
    "    # 因子数据标准化\n",
    "    print('Standard scaling.')\n",
    "    scaler = StandardScaler()\n",
    "    total_ts_train_val1 = np.apply_along_axis(\n",
    "        lambda x: np.clip(x, np.percentile(x, 0.5), np.percentile(x, 99.5)), axis=0, arr=total_ts_train_val1.reshape(-1, factor_num)\n",
    "        )  # 去极值，保留0.5%-99.5%数据\n",
    "    total_ts_train_val1 = total_ts_train_val1.reshape(len(date_list_train), pid_num, factor_num)\n",
    "    total_ts_train_val1 = np.nan_to_num(scaler.fit_transform(total_ts_train_val1.reshape(-1, factor_num)).reshape(len(date_list_train), pid_num, factor_num), nan=0)\n",
    "    total_ts_test1 = np.apply_along_axis(\n",
    "        lambda x: np.clip(x, np.percentile(x, 0.5), np.percentile(x, 99.5)), axis=0, arr=total_ts_test1.reshape(-1, factor_num)\n",
    "        )\n",
    "    total_ts_test1 = total_ts_test1.reshape(len(date_list_test), pid_num, factor_num)\n",
    "    total_ts_test1 = np.nan_to_num(scaler.transform(total_ts_test1.reshape(-1, factor_num)).reshape(len(date_list_test), pid_num, factor_num), nan=0)\n",
    "    \n",
    "    # KFold 交叉验证（并行训练）\n",
    "    print('KFold training.')\n",
    "    kf = KFold(n_splits=total_train_num, shuffle=False)\n",
    "    processes = []\n",
    "    for train_num, index_tuple in enumerate(kf.split(total_ts_train_val1)):\n",
    "        p = mp.Process(\n",
    "            target=train_one_Fold, \n",
    "            args=(\n",
    "                round_num, train_num, index_tuple, main_folder_name,\n",
    "                total_ts_train_val1, total_label_train_val, total_group_train_val, date_list_train,\n",
    "                total_ts_test1, total_label_test, total_group_test, date_list_test,\n",
    "                correlation_df, seed_list, dt1, dt2, dt3, dt4, dt5,\n",
    "                factor_num, corr_thres, save_path, model_mode, multi_model\n",
    "                )\n",
    "            )\n",
    "        processes.append(p)\n",
    "        p.start()\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 保存测试数据\n",
    "    print('Save test data.')\n",
    "    total_test_output = []\n",
    "    for train_num in range(total_train_num):\n",
    "        test_name = 'test_output_ic' + str(round_num) + str(train_num) + '.pt'\n",
    "        test_path = os.path.join(save_path, test_name)\n",
    "        total_test_output.append(torch.load(test_path))\n",
    "        \n",
    "    total_test_path = os.path.join(save_path, total_test_name)\n",
    "    total_date_pid_path = os.path.join(save_path, total_date_pid_name)\n",
    "    \n",
    "    total_test_output = torch.stack(total_test_output)\n",
    "    weight_tensor = torch.tensor([0.1, 0.15, 0.2, 0.25, 0.3]).view(-1, *([1] * (total_test_output.dim() - 1)))\n",
    "    total_test_output = (total_test_output * weight_tensor).sum(dim=0)\n",
    "    torch.save(total_test_output, total_test_path)\n",
    "    \n",
    "    stocks = np.array(grouped_label.loc[20200102].index)\n",
    "    repeated_stocks = np.tile(stocks, len(date_list_test))\n",
    "    repeated_dates = np.repeat(date_list_test, len(stocks))\n",
    "    date_pid_test = np.column_stack((repeated_dates, repeated_stocks))\n",
    "    torch.save(date_pid_test, total_date_pid_path)\n",
    "    \n",
    "    del total_ts_train_val1\n",
    "    del total_ts_test1\n",
    "    del total_label_train_val\n",
    "    del total_label_test\n",
    "    del total_group_train_val\n",
    "    del total_group_test\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和测试\n",
    "\n",
    "```c\n",
    "Round 1. Train: 2020/07/01 2022/07/01 Validation: 2022/07/01 2022/12/30 Test: 2023/01/01 2023/07/01\n",
    "Round 2. Train: 2021/01/01 2023/01/01 Validation: 2023/01/01 2023/03/31 Test: 2023/04/01 2023/07/01\n",
    "Round 3. Train: 2021/04/01 2023/04/01 Validation: 2023/04/01 2023/06/30 Test: 2023/07/01 2023/10/01\n",
    "Round 4. Train: 2021/07/01 2023/07/01 Validation: 2023/07/01 2023/09/28 Test: 2023/10/01 2024/01/01\n",
    "Round 5. Train: 2021/10/01 2023/10/01 Validation: 2023/10/01 2023/12/29 Test: 2024/01/01 2024/04/01\n",
    "Round 6. Train: 2022/01/01 2024/01/01 Validation: 2024/01/01 2024/03/29 Test: 2024/04/01 2024/07/01\n",
    "Round 7. Train: 2022/04/01 2024/04/01 Validation: 2024/04/01 2024/06/28 Test: 2024/07/01 2024/10/01\n",
    "Round 8. Train: 2022/07/01 2024/07/01 Validation: 2024/07/01 2024/09/30 Test: 2024/10/01 2025/01/01\n",
    "Round 9. Train: 2022/10/01 2024/10/01 Validation: 2024/10/01 2024/12/31 Test: 2025/01/01 2025/02/21\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/home/datamake117/data/haris/DL/\" + main_folder_name\n",
    "os.makedirs(folder_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 9.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca561c5da52548af9ce8c2ef280c651a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_val_data:   0%|          | 0/532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b1ebf773ff401d805bc337f7efbc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_data:   0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-max scaling.\n",
      "Standard scaling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datamake117/.local/lib/python3.9/site-packages/numpy/lib/function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py:1051: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py:1056: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py:1076: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/numpy/lib/function_base.py:4655: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "2025/04/16 13:50:39 train.py INFO Period9, Train0, Train Period:20221001-20241001, Val Period:20241001-20241231, Test Period:20250101-20250401\n",
      "2025/04/16 13:50:39 train.py INFO Train1 Shape: torch.Size([425, 5269, 2790]), Val1 Shape: torch.Size([106, 5269, 2790]), Test1 Shape: torch.Size([57, 5269, 2790])\n",
      "2025/04/16 13:50:39 train.py INFO Start Training\n",
      "2025/04/16 13:52:25 train.py INFO Epoch[1/200], Time:105.43sec, Train Loss: 0.926712, Val Loss: 0.8947311043739319,0.895044207572937,0.9068646430969238,0.8904220461845398,0.8923168778419495,0.8951656222343445\n",
      "2025/04/16 13:52:25 model.py INFO Validation loss decreased (inf --> 0.894731).  Saving model 0.0...\n",
      "2025/04/16 13:52:25 model.py INFO Validation loss decreased (inf --> 0.895044).  Saving model 1.0...\n",
      "2025/04/16 13:52:25 model.py INFO Validation loss decreased (inf --> 0.906865).  Saving model 2.0...\n",
      "2025/04/16 13:52:25 model.py INFO Validation loss decreased (inf --> 0.890422).  Saving model 3.0...\n",
      "2025/04/16 13:52:25 model.py INFO Validation loss decreased (inf --> 0.892317).  Saving model 4.0...\n",
      "2025/04/16 13:52:25 model.py INFO Validation loss decreased (inf --> 0.895166).  Saving model 5.0...\n",
      "2025/04/16 13:54:07 train.py INFO Epoch[2/200], Time:102.35sec, Train Loss: 0.893359, Val Loss: 0.8815166354179382,0.892806351184845,0.9125765562057495,0.886468768119812,0.8976970314979553,0.9078724384307861\n",
      "2025/04/16 13:54:07 model.py INFO Validation loss decreased (0.894731 --> 0.881517).  Saving model 0.0...\n",
      "2025/04/16 13:54:07 model.py INFO Validation loss decreased (0.895044 --> 0.892806).  Saving model 1.0...\n",
      "2025/04/16 13:54:07 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 13:54:07 model.py INFO Validation loss decreased (0.890422 --> 0.886469).  Saving model 3.0...\n",
      "2025/04/16 13:54:07 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 13:54:07 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "2025/04/16 13:56:04 train.py INFO Epoch[3/200], Time:116.36sec, Train Loss: 0.880350, Val Loss: 0.8838126063346863,0.8980969190597534,0.916814386844635,0.8883084654808044,0.8944602012634277,0.9102292060852051\n",
      "2025/04/16 13:56:04 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 13:56:04 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 13:56:04 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 13:56:04 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 13:56:04 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 13:56:04 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 13:56:05 train.py INFO Period9, Train1, Train Period:20221001-20241001, Val Period:20241001-20241231, Test Period:20250101-20250401\n",
      "2025/04/16 13:56:05 train.py INFO Train1 Shape: torch.Size([425, 5269, 2790]), Val1 Shape: torch.Size([106, 5269, 2790]), Test1 Shape: torch.Size([57, 5269, 2790])\n",
      "2025/04/16 13:56:05 train.py INFO Start Training\n",
      "2025/04/16 13:58:01 train.py INFO Epoch[4/200], Time:117.62sec, Train Loss: 0.871000, Val Loss: 0.8860042691230774,0.8956852555274963,0.9197808504104614,0.8928708434104919,0.8903751969337463,0.91892409324646\n",
      "2025/04/16 13:58:01 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 13:58:01 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 13:58:01 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 13:58:01 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 13:58:01 model.py INFO Validation loss decreased (0.892317 --> 0.890375).  Saving model 4.0...\n",
      "2025/04/16 13:58:01 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 13:59:13 train.py INFO Epoch[1/200], Time:188.12sec, Train Loss: 0.914231, Val Loss: 0.9103979468345642,0.9136781692504883,0.9236111044883728,0.9140034317970276,0.9071046113967896,0.9192296266555786\n",
      "2025/04/16 13:59:13 model.py INFO Validation loss decreased (inf --> 0.910398).  Saving model 0.0...\n",
      "2025/04/16 13:59:13 model.py INFO Validation loss decreased (inf --> 0.913678).  Saving model 1.0...\n",
      "2025/04/16 13:59:13 model.py INFO Validation loss decreased (inf --> 0.923611).  Saving model 2.0...\n",
      "2025/04/16 13:59:13 model.py INFO Validation loss decreased (inf --> 0.914003).  Saving model 3.0...\n",
      "2025/04/16 13:59:13 model.py INFO Validation loss decreased (inf --> 0.907105).  Saving model 4.0...\n",
      "2025/04/16 13:59:13 model.py INFO Validation loss decreased (inf --> 0.919230).  Saving model 5.0...\n",
      "2025/04/16 13:59:59 train.py INFO Epoch[5/200], Time:117.58sec, Train Loss: 0.862576, Val Loss: 0.8932715654373169,0.8927945494651794,0.9215945601463318,0.8970128297805786,0.8865638971328735,0.9212881922721863\n",
      "2025/04/16 13:59:59 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 13:59:59 model.py INFO Validation loss decreased (0.892806 --> 0.892795).  Saving model 1.0...\n",
      "2025/04/16 13:59:59 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 13:59:59 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 13:59:59 model.py INFO Validation loss decreased (0.890375 --> 0.886564).  Saving model 4.0...\n",
      "2025/04/16 13:59:59 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "2025/04/16 14:01:40 train.py INFO Epoch[2/200], Time:146.75sec, Train Loss: 0.882875, Val Loss: 0.9095348119735718,0.9116549491882324,0.9357093572616577,0.9186957478523254,0.9094300270080566,0.9264114499092102\n",
      "2025/04/16 14:01:40 model.py INFO Validation loss decreased (0.910398 --> 0.909535).  Saving model 0.0...\n",
      "2025/04/16 14:01:40 model.py INFO Validation loss decreased (0.913678 --> 0.911655).  Saving model 1.0...\n",
      "2025/04/16 14:01:40 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:01:40 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:01:40 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:01:40 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:01:40 train.py INFO Period9, Train2, Train Period:20221001-20241001, Val Period:20241001-20241231, Test Period:20250101-20250401\n",
      "2025/04/16 14:01:40 train.py INFO Train1 Shape: torch.Size([426, 5269, 2790]), Val1 Shape: torch.Size([105, 5269, 2790]), Test1 Shape: torch.Size([57, 5269, 2790])\n",
      "2025/04/16 14:01:40 train.py INFO Start Training\n",
      "2025/04/16 14:02:33 train.py INFO Epoch[6/200], Time:154.03sec, Train Loss: 0.854449, Val Loss: 0.8923644423484802,0.8917790055274963,0.9433232545852661,0.8938683867454529,0.8905603885650635,0.9101959466934204\n",
      "2025/04/16 14:02:33 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:02:33 model.py INFO Validation loss decreased (0.892795 --> 0.891779).  Saving model 1.0...\n",
      "2025/04/16 14:02:33 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:02:33 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:02:33 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:02:33 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:03:59 train.py INFO Epoch[3/200], Time:139.46sec, Train Loss: 0.870460, Val Loss: 0.918339729309082,0.9111474752426147,0.9432132840156555,0.9235280752182007,0.9072803854942322,0.9543394446372986\n",
      "2025/04/16 14:03:59 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:03:59 model.py INFO Validation loss decreased (0.911655 --> 0.911147).  Saving model 1.0...\n",
      "2025/04/16 14:03:59 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:03:59 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:03:59 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:03:59 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:04:00 train.py INFO Epoch[1/200], Time:139.83sec, Train Loss: 0.921285, Val Loss: 0.9002110362052917,0.9142522811889648,0.9031311273574829,0.903978705406189,0.915332019329071,0.907086193561554\n",
      "2025/04/16 14:04:00 model.py INFO Validation loss decreased (inf --> 0.900211).  Saving model 0.0...\n",
      "2025/04/16 14:04:00 model.py INFO Validation loss decreased (inf --> 0.914252).  Saving model 1.0...\n",
      "2025/04/16 14:04:00 model.py INFO Validation loss decreased (inf --> 0.903131).  Saving model 2.0...\n",
      "2025/04/16 14:04:00 model.py INFO Validation loss decreased (inf --> 0.903979).  Saving model 3.0...\n",
      "2025/04/16 14:04:00 model.py INFO Validation loss decreased (inf --> 0.915332).  Saving model 4.0...\n",
      "2025/04/16 14:04:00 model.py INFO Validation loss decreased (inf --> 0.907086).  Saving model 5.0...\n",
      "2025/04/16 14:05:30 train.py INFO Epoch[7/200], Time:177.49sec, Train Loss: 0.845752, Val Loss: 0.8910701870918274,0.8972792029380798,0.9534593820571899,0.8940427303314209,0.8917978405952454,0.9146673083305359\n",
      "2025/04/16 14:05:30 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:05:30 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:05:30 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:05:30 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:05:30 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:05:30 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:06:23 train.py INFO Epoch[4/200], Time:143.29sec, Train Loss: 0.861104, Val Loss: 0.920741081237793,0.9119312763214111,0.9543684720993042,0.9254052639007568,0.9129469394683838,0.9668666124343872\n",
      "2025/04/16 14:06:23 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:06:23 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:06:23 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:06:23 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:06:23 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:06:23 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:06:27 train.py INFO Epoch[2/200], Time:147.02sec, Train Loss: 0.890123, Val Loss: 0.8969953656196594,0.9210781455039978,0.8979446291923523,0.9002176523208618,0.9269556999206543,0.9010032415390015\n",
      "2025/04/16 14:06:27 model.py INFO Validation loss decreased (0.900211 --> 0.896995).  Saving model 0.0...\n",
      "2025/04/16 14:06:27 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:06:27 model.py INFO Validation loss decreased (0.903131 --> 0.897945).  Saving model 2.0...\n",
      "2025/04/16 14:06:27 model.py INFO Validation loss decreased (0.903979 --> 0.900218).  Saving model 3.0...\n",
      "2025/04/16 14:06:27 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:06:27 model.py INFO Validation loss decreased (0.907086 --> 0.901003).  Saving model 5.0...\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "2025/04/16 14:08:42 train.py INFO Period9, Train3, Train Period:20221001-20241001, Val Period:20241001-20241231, Test Period:20250101-20250401\n",
      "2025/04/16 14:08:42 train.py INFO Train1 Shape: torch.Size([426, 5269, 2790]), Val1 Shape: torch.Size([105, 5269, 2790]), Test1 Shape: torch.Size([57, 5269, 2790])\n",
      "2025/04/16 14:08:42 train.py INFO Start Training\n",
      "2025/04/16 14:10:28 train.py INFO Epoch[1/200], Time:105.53sec, Train Loss: 0.915348, Val Loss: 0.935706377029419,0.938920259475708,0.9286123514175415,0.9355533719062805,0.9370461106300354,0.9337875843048096\n",
      "2025/04/16 14:10:28 model.py INFO Validation loss decreased (inf --> 0.935706).  Saving model 0.0...\n",
      "2025/04/16 14:10:28 model.py INFO Validation loss decreased (inf --> 0.938920).  Saving model 1.0...\n",
      "2025/04/16 14:10:28 model.py INFO Validation loss decreased (inf --> 0.928612).  Saving model 2.0...\n",
      "2025/04/16 14:10:28 model.py INFO Validation loss decreased (inf --> 0.935553).  Saving model 3.0...\n",
      "2025/04/16 14:10:28 model.py INFO Validation loss decreased (inf --> 0.937046).  Saving model 4.0...\n",
      "2025/04/16 14:10:28 model.py INFO Validation loss decreased (inf --> 0.933788).  Saving model 5.0...\n",
      "2025/04/16 14:12:06 train.py INFO Epoch[2/200], Time:98.61sec, Train Loss: 0.882669, Val Loss: 0.9349690079689026,0.9540082812309265,0.961736261844635,0.9393360614776611,0.9562331438064575,0.9513862133026123\n",
      "2025/04/16 14:12:06 model.py INFO Validation loss decreased (0.935706 --> 0.934969).  Saving model 0.0...\n",
      "2025/04/16 14:12:06 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:12:06 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:12:06 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:12:06 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:12:06 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:13:49 train.py INFO Epoch[3/200], Time:102.63sec, Train Loss: 0.870021, Val Loss: 0.9437442421913147,0.9606407284736633,0.9726912379264832,0.9393123388290405,0.9643377661705017,0.9787406921386719\n",
      "2025/04/16 14:13:49 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:13:49 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:13:49 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:13:49 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:13:49 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:13:49 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:15:29 train.py INFO Epoch[4/200], Time:99.62sec, Train Loss: 0.860894, Val Loss: 0.9416618943214417,0.9630922675132751,0.979404866695404,0.9387905597686768,0.956899881362915,0.9780368804931641\n",
      "2025/04/16 14:15:29 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:15:29 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:15:29 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:15:29 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:15:29 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:15:29 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:16:52 train.py INFO Epoch[8/200], Time:681.86sec, Train Loss: 0.836179, Val Loss: 0.8861125707626343,0.8938769698143005,0.9612343311309814,0.8933148384094238,0.8942144513130188,0.9184643030166626\n",
      "2025/04/16 14:16:52 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:16:52 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:16:52 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:16:52 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:16:52 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:16:52 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:16:58 train.py INFO Epoch[3/200], Time:630.79sec, Train Loss: 0.877661, Val Loss: 0.8937177658081055,0.9259285926818848,0.8988023996353149,0.9063447713851929,0.931501567363739,0.9020665884017944\n",
      "2025/04/16 14:16:58 model.py INFO Validation loss decreased (0.896995 --> 0.893718).  Saving model 0.0...\n",
      "2025/04/16 14:16:58 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:16:58 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:16:58 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:16:58 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:16:58 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:17:44 train.py INFO Epoch[5/200], Time:135.47sec, Train Loss: 0.852483, Val Loss: 0.9354821443557739,0.9706043601036072,0.9759883284568787,0.9335430264472961,0.9543922543525696,0.9767179489135742\n",
      "2025/04/16 14:17:44 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:17:44 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:17:44 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:17:44 model.py INFO Validation loss decreased (0.935553 --> 0.933543).  Saving model 3.0...\n",
      "2025/04/16 14:17:44 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:17:44 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:17:58 train.py INFO Epoch[5/200], Time:695.66sec, Train Loss: 0.852267, Val Loss: 0.9248857498168945,0.9165517091751099,0.9588426351547241,0.9397096633911133,0.9179816842079163,0.9762985110282898\n",
      "2025/04/16 14:17:58 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:17:58 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:17:58 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:17:58 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:17:58 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:17:58 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "/home/datamake117/.local/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "2025/04/16 14:19:17 train.py INFO Period9, Train4, Train Period:20221001-20241001, Val Period:20241001-20241231, Test Period:20250101-20250401\n",
      "2025/04/16 14:19:17 train.py INFO Train1 Shape: torch.Size([426, 5269, 2790]), Val1 Shape: torch.Size([105, 5269, 2790]), Test1 Shape: torch.Size([57, 5269, 2790])\n",
      "2025/04/16 14:19:17 train.py INFO Start Training\n",
      "2025/04/16 14:19:53 train.py INFO Epoch[4/200], Time:174.75sec, Train Loss: 0.867830, Val Loss: 0.8921743631362915,0.9425899982452393,0.9055465459823608,0.9056779146194458,0.9382728338241577,0.9037580490112305\n",
      "2025/04/16 14:19:53 model.py INFO Validation loss decreased (0.893718 --> 0.892174).  Saving model 0.0...\n",
      "2025/04/16 14:19:53 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:19:53 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:19:53 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:19:53 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:19:53 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:19:59 train.py INFO Epoch[9/200], Time:186.91sec, Train Loss: 0.825270, Val Loss: 0.892280638217926,0.8949838876724243,0.9574326276779175,0.8990567922592163,0.8945622444152832,0.9398189783096313\n",
      "2025/04/16 14:19:59 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:19:59 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:19:59 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:19:59 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:19:59 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:19:59 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:20:44 train.py INFO Epoch[6/200], Time:180.24sec, Train Loss: 0.844164, Val Loss: 0.9328633546829224,0.9626128077507019,0.9836735129356384,0.9359437823295593,0.9470144510269165,0.99314945936203\n",
      "2025/04/16 14:20:44 model.py INFO Validation loss decreased (0.934969 --> 0.932863).  Saving model 0.0...\n",
      "2025/04/16 14:20:44 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:20:44 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:20:44 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:20:44 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:20:44 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:21:07 train.py INFO Epoch[6/200], Time:188.16sec, Train Loss: 0.843615, Val Loss: 0.9261876344680786,0.9166402816772461,0.983506977558136,0.9418721199035645,0.9202805161476135,1.002245306968689\n",
      "2025/04/16 14:21:07 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:21:07 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:21:07 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:21:07 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:21:07 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:21:07 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:22:26 train.py INFO Epoch[1/200], Time:188.61sec, Train Loss: 0.922231, Val Loss: 0.9022719264030457,0.9018169641494751,0.9090078473091125,0.8995862007141113,0.901807963848114,0.9066418409347534\n",
      "2025/04/16 14:22:26 model.py INFO Validation loss decreased (inf --> 0.902272).  Saving model 0.0...\n",
      "2025/04/16 14:22:26 model.py INFO Validation loss decreased (inf --> 0.901817).  Saving model 1.0...\n",
      "2025/04/16 14:22:26 model.py INFO Validation loss decreased (inf --> 0.909008).  Saving model 2.0...\n",
      "2025/04/16 14:22:26 model.py INFO Validation loss decreased (inf --> 0.899586).  Saving model 3.0...\n",
      "2025/04/16 14:22:26 model.py INFO Validation loss decreased (inf --> 0.901808).  Saving model 4.0...\n",
      "2025/04/16 14:22:26 model.py INFO Validation loss decreased (inf --> 0.906642).  Saving model 5.0...\n",
      "2025/04/16 14:22:56 train.py INFO Epoch[5/200], Time:182.89sec, Train Loss: 0.859140, Val Loss: 0.8943758606910706,0.9407963156700134,0.9023365378379822,0.9134108424186707,0.9443781971931458,0.9071162343025208\n",
      "2025/04/16 14:22:56 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:22:56 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:22:56 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:22:56 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:22:56 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:22:56 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:23:06 train.py INFO Epoch[10/200], Time:187.06sec, Train Loss: 0.812210, Val Loss: 0.9009575843811035,0.9036567807197571,0.9567172527313232,0.9051106572151184,0.9021382331848145,0.9333621263504028\n",
      "2025/04/16 14:23:06 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:23:06 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:23:06 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:23:06 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:23:06 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:23:06 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:23:49 train.py INFO Epoch[7/200], Time:185.00sec, Train Loss: 0.834930, Val Loss: 0.9367541074752808,0.9607182145118713,0.9847356081008911,0.9304773211479187,0.9488765597343445,0.9978895783424377\n",
      "2025/04/16 14:23:49 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:23:49 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:23:49 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:23:49 model.py INFO Validation loss decreased (0.933543 --> 0.930477).  Saving model 3.0...\n",
      "2025/04/16 14:23:49 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:23:49 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:24:09 train.py INFO Epoch[7/200], Time:182.07sec, Train Loss: 0.834379, Val Loss: 0.9276584982872009,0.9190500974655151,0.9990437626838684,0.9408507347106934,0.922947108745575,0.9978916049003601\n",
      "2025/04/16 14:24:09 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:24:09 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:24:09 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:24:09 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:24:09 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:24:09 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:25:30 train.py INFO Epoch[2/200], Time:184.31sec, Train Loss: 0.890662, Val Loss: 0.8939375281333923,0.8905606269836426,0.9063940644264221,0.8936219215393066,0.8910642862319946,0.904209554195404\n",
      "2025/04/16 14:25:30 model.py INFO Validation loss decreased (0.902272 --> 0.893938).  Saving model 0.0...\n",
      "2025/04/16 14:25:30 model.py INFO Validation loss decreased (0.901817 --> 0.890561).  Saving model 1.0...\n",
      "2025/04/16 14:25:30 model.py INFO Validation loss decreased (0.909008 --> 0.906394).  Saving model 2.0...\n",
      "2025/04/16 14:25:30 model.py INFO Validation loss decreased (0.899586 --> 0.893622).  Saving model 3.0...\n",
      "2025/04/16 14:25:30 model.py INFO Validation loss decreased (0.901808 --> 0.891064).  Saving model 4.0...\n",
      "2025/04/16 14:25:30 model.py INFO Validation loss decreased (0.906642 --> 0.904210).  Saving model 5.0...\n",
      "2025/04/16 14:26:03 train.py INFO Epoch[6/200], Time:187.12sec, Train Loss: 0.851068, Val Loss: 0.9079938530921936,0.9497469067573547,0.9036160707473755,0.9176751971244812,0.9416496753692627,0.9115281105041504\n",
      "2025/04/16 14:26:03 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:26:03 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:26:03 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:26:03 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:26:03 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:26:03 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:26:10 train.py INFO Epoch[11/200], Time:183.68sec, Train Loss: 0.796251, Val Loss: 0.9139073491096497,0.9036432504653931,0.9498096704483032,0.9225394129753113,0.9061411619186401,0.9434857964515686\n",
      "2025/04/16 14:26:10 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:26:10 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:26:10 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:26:10 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:26:10 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:26:10 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:26:52 train.py INFO Epoch[8/200], Time:182.14sec, Train Loss: 0.824443, Val Loss: 0.9330071210861206,0.9648442268371582,1.0044008493423462,0.9347907304763794,0.9578871130943298,1.0212358236312866\n",
      "2025/04/16 14:26:52 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:26:52 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:26:52 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:26:52 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:26:52 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:26:52 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:27:08 train.py INFO Epoch[8/200], Time:179.32sec, Train Loss: 0.824229, Val Loss: 0.9300979375839233,0.9235085248947144,1.0100491046905518,0.9382929801940918,0.9202216267585754,0.9864462018013\n",
      "2025/04/16 14:27:08 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:27:08 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:27:08 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:27:08 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:27:08 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:27:08 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:28:17 train.py INFO Epoch[12/200], Time:126.62sec, Train Loss: 0.824728, Val Loss: 0.9186604022979736,0.9083547592163086,0.9498096704483032,0.9292572736740112,0.9061002135276794,0.9434857964515686\n",
      "2025/04/16 14:28:17 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:28:17 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:28:17 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:28:17 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:28:17 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:28:17 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:28:33 train.py INFO Epoch[3/200], Time:182.28sec, Train Loss: 0.878263, Val Loss: 0.890364944934845,0.8904575109481812,0.9012504816055298,0.8904919624328613,0.8901822566986084,0.906745195388794\n",
      "2025/04/16 14:28:33 model.py INFO Validation loss decreased (0.893938 --> 0.890365).  Saving model 0.0...\n",
      "2025/04/16 14:28:33 model.py INFO Validation loss decreased (0.890561 --> 0.890458).  Saving model 1.0...\n",
      "2025/04/16 14:28:33 model.py INFO Validation loss decreased (0.906394 --> 0.901250).  Saving model 2.0...\n",
      "2025/04/16 14:28:33 model.py INFO Validation loss decreased (0.893622 --> 0.890492).  Saving model 3.0...\n",
      "2025/04/16 14:28:33 model.py INFO Validation loss decreased (0.891064 --> 0.890182).  Saving model 4.0...\n",
      "2025/04/16 14:28:33 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:29:03 train.py INFO Epoch[7/200], Time:179.89sec, Train Loss: 0.842201, Val Loss: 0.9066020846366882,0.9474157691001892,0.9059800505638123,0.9238195419311523,0.9467889070510864,0.9281740784645081\n",
      "2025/04/16 14:29:03 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:29:03 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:29:03 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:29:03 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:29:03 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:29:03 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:29:54 train.py INFO Epoch[13/200], Time:97.64sec, Train Loss: 0.850320, Val Loss: 0.9186604022979736,0.9155603051185608,0.9498096704483032,0.9292572736740112,0.917280375957489,0.9434857964515686\n",
      "2025/04/16 14:29:54 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:29:54 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:29:54 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:29:54 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:29:54 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:29:54 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:29:55 train.py INFO Epoch[9/200], Time:183.48sec, Train Loss: 0.812541, Val Loss: 0.9315835237503052,0.9620454907417297,0.9938782453536987,0.9268242120742798,0.9576122760772705,1.0203815698623657\n",
      "2025/04/16 14:29:55 model.py INFO Validation loss decreased (0.932863 --> 0.931584).  Saving model 0.0...\n",
      "2025/04/16 14:29:55 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:29:55 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:29:55 model.py INFO Validation loss decreased (0.930477 --> 0.926824).  Saving model 3.0...\n",
      "2025/04/16 14:29:55 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:29:55 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:30:11 train.py INFO Epoch[9/200], Time:183.48sec, Train Loss: 0.812599, Val Loss: 0.9344912767410278,0.9181368350982666,1.0132300853729248,0.9422710537910461,0.9204626679420471,0.9846767783164978\n",
      "2025/04/16 14:30:11 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:30:11 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:30:11 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:30:11 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:30:11 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:30:11 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:31:32 train.py INFO Epoch[14/200], Time:97.79sec, Train Loss: 0.843079, Val Loss: 0.9186604022979736,0.917127251625061,0.9498096704483032,0.9292572736740112,0.9232445955276489,0.9434857964515686\n",
      "2025/04/16 14:31:32 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:31:32 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:31:32 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:31:32 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:31:32 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:31:32 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:31:39 train.py INFO Epoch[4/200], Time:185.89sec, Train Loss: 0.868750, Val Loss: 0.8894331455230713,0.8885549306869507,0.9037593007087708,0.8879109025001526,0.8918980360031128,0.9206742644309998\n",
      "2025/04/16 14:31:39 model.py INFO Validation loss decreased (0.890365 --> 0.889433).  Saving model 0.0...\n",
      "2025/04/16 14:31:39 model.py INFO Validation loss decreased (0.890458 --> 0.888555).  Saving model 1.0...\n",
      "2025/04/16 14:31:39 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:31:39 model.py INFO Validation loss decreased (0.890492 --> 0.887911).  Saving model 3.0...\n",
      "2025/04/16 14:31:39 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:31:39 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:32:08 train.py INFO Epoch[8/200], Time:185.21sec, Train Loss: 0.832142, Val Loss: 0.9247965216636658,0.9462031722068787,0.9189066886901855,0.9311677813529968,0.9491559267044067,0.9269412159919739\n",
      "2025/04/16 14:32:08 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:32:08 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:32:08 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:32:08 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:32:08 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:32:08 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:32:57 train.py INFO Epoch[10/200], Time:182.18sec, Train Loss: 0.797516, Val Loss: 0.9259974956512451,0.967555046081543,0.9955211877822876,0.9295035004615784,0.9599863290786743,1.0266509056091309\n",
      "2025/04/16 14:32:57 model.py INFO Validation loss decreased (0.931584 --> 0.925997).  Saving model 0.0...\n",
      "2025/04/16 14:32:57 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:32:57 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:32:57 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:32:57 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:32:57 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:33:08 train.py INFO Epoch[15/200], Time:95.97sec, Train Loss: 0.835238, Val Loss: 0.9186604022979736,0.9151373505592346,0.9498096704483032,0.9292572736740112,0.9209266901016235,0.9434857964515686\n",
      "2025/04/16 14:33:08 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:33:08 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:33:08 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:33:08 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:33:08 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:33:08 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:33:17 train.py INFO Epoch[10/200], Time:185.87sec, Train Loss: 0.798577, Val Loss: 0.9307881593704224,0.9211490750312805,1.0031449794769287,0.9404246807098389,0.9213333129882812,0.9680483937263489\n",
      "2025/04/16 14:33:17 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:33:17 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:33:17 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:33:17 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:33:17 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:33:17 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:34:16 train.py INFO Epoch[16/200], Time:68.56sec, Train Loss: 0.860218, Val Loss: 0.9186604022979736,0.9209932088851929,0.9498096704483032,0.9292572736740112,0.9209266901016235,0.9434857964515686\n",
      "2025/04/16 14:34:16 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:34:16 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:34:16 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:34:16 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:34:16 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:34:16 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:34:16 train.py INFO Early stopping\n",
      "2025/04/16 14:34:16 train.py INFO Val Loss: 0.8815166354179382,0.8917790055274963,0.9068646430969238,0.886468768119812,0.8865638971328735,0.8951656222343445\n",
      "2025/04/16 14:34:27 train.py INFO Test Loss: 0.046929\n",
      "2025/04/16 14:34:31 train.py INFO Finish 0 Fold.\n",
      "2025/04/16 14:34:44 train.py INFO Epoch[5/200], Time:184.90sec, Train Loss: 0.860266, Val Loss: 0.8874062299728394,0.8922401666641235,0.9076027274131775,0.8871719837188721,0.8937963843345642,0.9242353439331055\n",
      "2025/04/16 14:34:44 model.py INFO Validation loss decreased (0.889433 --> 0.887406).  Saving model 0.0...\n",
      "2025/04/16 14:34:44 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:34:44 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:34:44 model.py INFO Validation loss decreased (0.887911 --> 0.887172).  Saving model 3.0...\n",
      "2025/04/16 14:34:44 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:34:44 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:35:09 train.py INFO Epoch[9/200], Time:181.13sec, Train Loss: 0.820571, Val Loss: 0.9213590621948242,0.9553513526916504,0.9190113544464111,0.9326708912849426,0.9318854808807373,0.9334585070610046\n",
      "2025/04/16 14:35:09 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:35:09 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:35:09 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:35:09 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:35:09 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:35:09 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:35:50 train.py INFO Epoch[11/200], Time:172.63sec, Train Loss: 0.778503, Val Loss: 0.9262136220932007,0.9757351279258728,1.0001294612884521,0.9215467572212219,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:35:50 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:35:50 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:35:50 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:35:50 model.py INFO Validation loss decreased (0.926824 --> 0.921547).  Saving model 3.0...\n",
      "2025/04/16 14:35:50 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:35:50 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:36:08 train.py INFO Epoch[11/200], Time:170.29sec, Train Loss: 0.780974, Val Loss: 0.9221689105033875,0.9238925576210022,1.00623619556427,0.926551103591919,0.9238343238830566,0.986029863357544\n",
      "2025/04/16 14:36:08 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:36:08 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:36:08 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:36:08 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:36:08 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:36:08 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:37:01 train.py INFO Epoch[12/200], Time:70.65sec, Train Loss: 0.881280, Val Loss: 0.9224913716316223,0.9757351279258728,1.0001294612884521,0.9319783449172974,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:37:01 model.py INFO Validation loss decreased (0.925997 --> 0.922491).  Saving model 0.0...\n",
      "2025/04/16 14:37:01 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:01 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:01 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:37:01 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:01 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:27 train.py INFO Epoch[6/200], Time:162.76sec, Train Loss: 0.851552, Val Loss: 0.8887525200843811,0.8933919668197632,0.9249690771102905,0.8898177742958069,0.898501992225647,0.9332242608070374\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:37:27 train.py INFO Epoch[12/200], Time:79.64sec, Train Loss: 0.866430, Val Loss: 0.9230600595474243,0.9232195019721985,1.00623619556427,0.926551103591919,0.9238343238830566,0.986029863357544\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:27 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:37:51 train.py INFO Epoch[10/200], Time:162.25sec, Train Loss: 0.806921, Val Loss: 0.9237176775932312,0.9552265405654907,0.9297264814376831,0.9346771836280823,0.9420915246009827,0.9316824674606323\n",
      "2025/04/16 14:37:51 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:37:51 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:37:51 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:37:51 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:37:51 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:37:51 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:38:11 train.py INFO Epoch[13/200], Time:70.47sec, Train Loss: 0.874313, Val Loss: 0.9305512309074402,0.9757351279258728,1.0001294612884521,0.9384036064147949,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:38:11 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:38:11 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:11 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:11 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:38:11 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:11 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:30 train.py INFO Epoch[13/200], Time:62.88sec, Train Loss: 0.887058, Val Loss: 0.9230600595474243,0.939853310585022,1.00623619556427,0.926551103591919,0.9238343238830566,0.986029863357544\n",
      "2025/04/16 14:38:30 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:38:30 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:38:30 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:30 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:30 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:30 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:38:30 train.py INFO Early stopping\n",
      "2025/04/16 14:38:30 train.py INFO Val Loss: 0.9095348119735718,0.9111474752426147,0.9236111044883728,0.9140034317970276,0.9071046113967896,0.9192296266555786\n",
      "2025/04/16 14:38:40 train.py INFO Test Loss: 0.052619\n",
      "2025/04/16 14:38:44 train.py INFO Finish 1 Fold.\n",
      "2025/04/16 14:39:14 train.py INFO Epoch[14/200], Time:63.28sec, Train Loss: 0.867001, Val Loss: 0.933252215385437,0.9757351279258728,1.0001294612884521,0.9561498761177063,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:39:14 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:39:14 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:39:14 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:39:14 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:39:14 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:39:14 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:39:54 train.py INFO Epoch[7/200], Time:147.07sec, Train Loss: 0.841747, Val Loss: 0.8844138979911804,0.8959906697273254,0.9285523891448975,0.8919042348861694,0.8996996879577637,0.937037467956543\n",
      "2025/04/16 14:39:54 model.py INFO Validation loss decreased (0.887406 --> 0.884414).  Saving model 0.0...\n",
      "2025/04/16 14:39:54 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:39:54 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:39:54 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:39:54 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:39:54 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:40:16 train.py INFO Epoch[15/200], Time:61.97sec, Train Loss: 0.861985, Val Loss: 0.928053617477417,0.9757351279258728,1.0001294612884521,0.9588395357131958,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:40:16 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:40:16 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:40:16 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:40:16 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:40:16 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:40:16 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:40:17 train.py INFO Epoch[11/200], Time:145.93sec, Train Loss: 0.789758, Val Loss: 0.9295650720596313,0.9568261504173279,0.9225001335144043,0.9408116936683655,0.9544587135314941,0.9291197061538696\n",
      "2025/04/16 14:40:17 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:40:17 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:40:17 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:40:17 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:40:17 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:40:17 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:41:17 train.py INFO Epoch[16/200], Time:60.25sec, Train Loss: 0.857286, Val Loss: 0.9346705675125122,0.9757351279258728,1.0001294612884521,0.9504086971282959,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:41:17 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:41:17 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:41:17 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:41:17 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:41:17 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:41:17 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:41:59 train.py INFO Epoch[12/200], Time:101.42sec, Train Loss: 0.829628, Val Loss: 0.9338070154190063,0.9568261504173279,0.9389747381210327,0.9421974420547485,0.9544587135314941,0.9359104037284851\n",
      "2025/04/16 14:41:59 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:41:59 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:41:59 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:41:59 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:41:59 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:41:59 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:42:16 train.py INFO Epoch[8/200], Time:142.21sec, Train Loss: 0.829914, Val Loss: 0.8861347436904907,0.8956605792045593,0.9334124326705933,0.8882502913475037,0.8981084823608398,0.9628193974494934\n",
      "2025/04/16 14:42:16 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:42:16 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:42:16 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:42:16 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:42:16 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:42:16 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:42:18 train.py INFO Epoch[17/200], Time:61.46sec, Train Loss: 0.852578, Val Loss: 0.9505440592765808,0.9757351279258728,1.0001294612884521,0.9421594142913818,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:42:18 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:42:18 model.py INFO EarlyStopping counter: 16 out of 10\n",
      "2025/04/16 14:42:18 model.py INFO EarlyStopping counter: 16 out of 10\n",
      "2025/04/16 14:42:18 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:42:18 model.py INFO EarlyStopping counter: 16 out of 10\n",
      "2025/04/16 14:42:18 model.py INFO EarlyStopping counter: 16 out of 10\n",
      "2025/04/16 14:42:50 train.py INFO Epoch[13/200], Time:51.05sec, Train Loss: 0.882766, Val Loss: 0.926281213760376,0.9568261504173279,0.9389747381210327,0.9421974420547485,0.9544587135314941,0.9359104037284851\n",
      "2025/04/16 14:42:50 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:42:50 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:42:50 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:42:50 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:42:50 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:42:50 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:43:22 train.py INFO Epoch[18/200], Time:63.49sec, Train Loss: 0.847043, Val Loss: 0.9516372680664062,0.9757351279258728,1.0001294612884521,0.9490246772766113,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:43:22 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:43:22 model.py INFO EarlyStopping counter: 17 out of 10\n",
      "2025/04/16 14:43:22 model.py INFO EarlyStopping counter: 17 out of 10\n",
      "2025/04/16 14:43:22 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:43:22 model.py INFO EarlyStopping counter: 17 out of 10\n",
      "2025/04/16 14:43:22 model.py INFO EarlyStopping counter: 17 out of 10\n",
      "2025/04/16 14:43:40 train.py INFO Epoch[14/200], Time:50.27sec, Train Loss: 0.879114, Val Loss: 0.9266015887260437,0.9568261504173279,0.9389747381210327,0.9421974420547485,0.9544587135314941,0.9359104037284851\n",
      "2025/04/16 14:43:40 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:43:40 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:43:40 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:43:40 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:43:40 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:43:40 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:43:40 train.py INFO Early stopping\n",
      "2025/04/16 14:43:40 train.py INFO Val Loss: 0.8921743631362915,0.9142522811889648,0.8979446291923523,0.9002176523208618,0.915332019329071,0.9010032415390015\n",
      "2025/04/16 14:43:47 train.py INFO Test Loss: 0.031636\n",
      "2025/04/16 14:43:50 train.py INFO Finish 2 Fold.\n",
      "2025/04/16 14:44:16 train.py INFO Epoch[19/200], Time:54.08sec, Train Loss: 0.842562, Val Loss: 0.9592525959014893,0.9757351279258728,1.0001294612884521,0.9356028437614441,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:44:16 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:44:16 model.py INFO EarlyStopping counter: 18 out of 10\n",
      "2025/04/16 14:44:16 model.py INFO EarlyStopping counter: 18 out of 10\n",
      "2025/04/16 14:44:16 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:44:16 model.py INFO EarlyStopping counter: 18 out of 10\n",
      "2025/04/16 14:44:16 model.py INFO EarlyStopping counter: 18 out of 10\n",
      "2025/04/16 14:44:27 train.py INFO Epoch[9/200], Time:131.08sec, Train Loss: 0.815553, Val Loss: 0.8872498273849487,0.8992376327514648,0.9373705387115479,0.8870678544044495,0.8956868648529053,0.9701873660087585\n",
      "2025/04/16 14:44:27 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:44:27 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:44:27 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:44:27 model.py INFO Validation loss decreased (0.887172 --> 0.887068).  Saving model 3.0...\n",
      "2025/04/16 14:44:27 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:44:27 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:45:05 train.py INFO Epoch[20/200], Time:49.13sec, Train Loss: 0.837179, Val Loss: 0.9632314443588257,0.9757351279258728,1.0001294612884521,0.9348745346069336,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:45:05 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:45:05 model.py INFO EarlyStopping counter: 19 out of 10\n",
      "2025/04/16 14:45:05 model.py INFO EarlyStopping counter: 19 out of 10\n",
      "2025/04/16 14:45:05 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:45:05 model.py INFO EarlyStopping counter: 19 out of 10\n",
      "2025/04/16 14:45:05 model.py INFO EarlyStopping counter: 19 out of 10\n",
      "2025/04/16 14:45:53 train.py INFO Epoch[21/200], Time:48.66sec, Train Loss: 0.832437, Val Loss: 0.9592767357826233,0.9757351279258728,1.0001294612884521,0.9409295320510864,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:45:53 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:45:53 model.py INFO EarlyStopping counter: 20 out of 10\n",
      "2025/04/16 14:45:53 model.py INFO EarlyStopping counter: 20 out of 10\n",
      "2025/04/16 14:45:53 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:45:53 model.py INFO EarlyStopping counter: 20 out of 10\n",
      "2025/04/16 14:45:53 model.py INFO EarlyStopping counter: 20 out of 10\n",
      "2025/04/16 14:46:24 train.py INFO Epoch[10/200], Time:116.98sec, Train Loss: 0.798381, Val Loss: 0.8880820274353027,0.904784083366394,0.9493725299835205,0.8905044198036194,0.9040877819061279,0.9574265480041504\n",
      "2025/04/16 14:46:24 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:46:24 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:46:24 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:46:24 model.py INFO EarlyStopping counter: 1 out of 10\n",
      "2025/04/16 14:46:24 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:46:24 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:46:32 train.py INFO Epoch[22/200], Time:38.54sec, Train Loss: 0.879050, Val Loss: 0.9556869268417358,0.9757351279258728,1.0001294612884521,0.9409295320510864,0.9643807411193848,1.0218017101287842\n",
      "2025/04/16 14:46:32 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:46:32 model.py INFO EarlyStopping counter: 21 out of 10\n",
      "2025/04/16 14:46:32 model.py INFO EarlyStopping counter: 21 out of 10\n",
      "2025/04/16 14:46:32 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:46:32 model.py INFO EarlyStopping counter: 21 out of 10\n",
      "2025/04/16 14:46:32 model.py INFO EarlyStopping counter: 21 out of 10\n",
      "2025/04/16 14:46:32 train.py INFO Early stopping\n",
      "2025/04/16 14:46:32 train.py INFO Val Loss: 0.9224913716316223,0.938920259475708,0.9286123514175415,0.9215467572212219,0.9370461106300354,0.9337875843048096\n",
      "2025/04/16 14:46:38 train.py INFO Test Loss: 0.040944\n",
      "2025/04/16 14:46:42 train.py INFO Finish 3 Fold.\n",
      "2025/04/16 14:48:02 train.py INFO Epoch[11/200], Time:97.99sec, Train Loss: 0.778084, Val Loss: 0.8895931839942932,0.9133248925209045,0.9533775448799133,0.8965109586715698,0.9061241149902344,0.9484480023384094\n",
      "2025/04/16 14:48:02 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:48:02 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:48:02 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:48:02 model.py INFO EarlyStopping counter: 2 out of 10\n",
      "2025/04/16 14:48:02 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:48:02 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:49:37 train.py INFO Epoch[12/200], Time:94.83sec, Train Loss: 0.755116, Val Loss: 0.9011567234992981,0.9140767455101013,0.9748943448066711,0.8991737365722656,0.9100874662399292,0.9379189014434814\n",
      "2025/04/16 14:49:37 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:49:37 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:49:37 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:49:37 model.py INFO EarlyStopping counter: 3 out of 10\n",
      "2025/04/16 14:49:37 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:49:37 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:50:59 train.py INFO Epoch[13/200], Time:81.80sec, Train Loss: 0.767151, Val Loss: 0.9112196564674377,0.9126418828964233,0.9692943096160889,0.9099210500717163,0.9183862209320068,0.9379189014434814\n",
      "2025/04/16 14:50:59 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:50:59 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:50:59 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:50:59 model.py INFO EarlyStopping counter: 4 out of 10\n",
      "2025/04/16 14:50:59 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:50:59 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:51:50 train.py INFO Epoch[14/200], Time:51.69sec, Train Loss: 0.809089, Val Loss: 0.9139677286148071,0.9297098517417908,0.9692943096160889,0.9205965995788574,0.9183862209320068,0.9379189014434814\n",
      "2025/04/16 14:51:50 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:51:50 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:51:50 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:51:50 model.py INFO EarlyStopping counter: 5 out of 10\n",
      "2025/04/16 14:51:50 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:51:50 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:52:26 train.py INFO Epoch[15/200], Time:35.93sec, Train Loss: 0.827534, Val Loss: 0.9160359501838684,0.9297098517417908,0.9692943096160889,0.9207041263580322,0.9183862209320068,0.9379189014434814\n",
      "2025/04/16 14:52:26 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:52:26 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:52:26 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:52:26 model.py INFO EarlyStopping counter: 6 out of 10\n",
      "2025/04/16 14:52:26 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:52:26 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:53:02 train.py INFO Epoch[16/200], Time:35.87sec, Train Loss: 0.822285, Val Loss: 0.91731196641922,0.9297098517417908,0.9692943096160889,0.9208071231842041,0.9183862209320068,0.9379189014434814\n",
      "2025/04/16 14:53:02 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:53:02 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:53:02 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:53:02 model.py INFO EarlyStopping counter: 7 out of 10\n",
      "2025/04/16 14:53:02 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:53:02 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:53:38 train.py INFO Epoch[17/200], Time:35.79sec, Train Loss: 0.817716, Val Loss: 0.9165897369384766,0.9297098517417908,0.9692943096160889,0.9256014227867126,0.9183862209320068,0.9379189014434814\n",
      "2025/04/16 14:53:38 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:53:38 model.py INFO EarlyStopping counter: 13 out of 10\n",
      "2025/04/16 14:53:38 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:53:38 model.py INFO EarlyStopping counter: 8 out of 10\n",
      "2025/04/16 14:53:38 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:53:38 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:54:03 train.py INFO Epoch[18/200], Time:24.83sec, Train Loss: 0.852428, Val Loss: 0.9165897369384766,0.9297098517417908,0.9692943096160889,0.9314895272254944,0.9183862209320068,0.9379189014434814\n",
      "2025/04/16 14:54:03 model.py INFO EarlyStopping counter: 11 out of 10\n",
      "2025/04/16 14:54:03 model.py INFO EarlyStopping counter: 14 out of 10\n",
      "2025/04/16 14:54:03 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:54:03 model.py INFO EarlyStopping counter: 9 out of 10\n",
      "2025/04/16 14:54:03 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:54:03 model.py INFO EarlyStopping counter: 16 out of 10\n",
      "2025/04/16 14:54:28 train.py INFO Epoch[19/200], Time:24.94sec, Train Loss: 0.850283, Val Loss: 0.9165897369384766,0.9297098517417908,0.9692943096160889,0.9257684946060181,0.9183862209320068,0.9379189014434814\n",
      "2025/04/16 14:54:28 model.py INFO EarlyStopping counter: 12 out of 10\n",
      "2025/04/16 14:54:28 model.py INFO EarlyStopping counter: 15 out of 10\n",
      "2025/04/16 14:54:28 model.py INFO EarlyStopping counter: 16 out of 10\n",
      "2025/04/16 14:54:28 model.py INFO EarlyStopping counter: 10 out of 10\n",
      "2025/04/16 14:54:28 model.py INFO EarlyStopping counter: 16 out of 10\n",
      "2025/04/16 14:54:28 model.py INFO EarlyStopping counter: 17 out of 10\n",
      "2025/04/16 14:54:28 train.py INFO Early stopping\n",
      "2025/04/16 14:54:28 train.py INFO Val Loss: 0.8844138979911804,0.8885549306869507,0.9012504816055298,0.8870678544044495,0.8901822566986084,0.904209554195404\n",
      "2025/04/16 14:54:33 train.py INFO Test Loss: 0.034457\n",
      "2025/04/16 14:54:37 train.py INFO Finish 4 Fold.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save test data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190568/2390507001.py:117: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  total_test_output.append(torch.load(test_path))\n",
      "/tmp/ipykernel_190568/3743245873.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_output = torch.load(\"/home/datamake117/data/haris/DL/\" + main_folder_name + \"/test_output_\" + str(round_num) + \".pt\")\n"
     ]
    }
   ],
   "source": [
    "# 第2-9轮\n",
    "total_date_list = np.array(factor['date'].drop_duplicates().tolist())\n",
    "rolling_step = 3    # 3个月滚动训练\n",
    "window_size = 24    # 训练集大小\n",
    "val_size = 3        # 验证集大小\n",
    "corr_thres = 0.9\n",
    "for round_num in range(2, 10):\n",
    "    if round_num < 9:\n",
    "        continue\n",
    "    print('Round %i.' % round_num)\n",
    "    start_date = pd.to_datetime('2021-01-01')\n",
    "    dt1 = start_date + relativedelta(months=rolling_step * (round_num - 2))             # 训练集开始时间\n",
    "    dt2 = dt1 + relativedelta(months=window_size)                                       # 验证集开始时间\n",
    "    dt3 = dt2 + relativedelta(months=val_size)                                          # 验证集结束时间\n",
    "    dt4 = dt3                                                                           # 测试集开始时间\n",
    "    dt5 = dt3 + relativedelta(months=rolling_step)                                      # 测试集结束时间\n",
    "    dt3 = total_date_list[total_date_list < int(dt3.strftime('%Y%m%d'))][-1]\n",
    "    dt1, dt2, dt3, dt4, dt5 = int(dt1.strftime('%Y%m%d')), int(dt2.strftime('%Y%m%d')), int(dt3), int(dt4.strftime('%Y%m%d')), int(dt5.strftime('%Y%m%d'))\n",
    "    main(\n",
    "        round_num, dt1, dt2, dt3, dt4, dt5,\n",
    "        correlation_df, grouped, grouped_label, grouped_liquidity,\n",
    "        total_date_list, main_folder_name, corr_thres=0.9, seed_num=5, model_mode=False\n",
    "        )\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "test_output_list = []\n",
    "for round_num in range(2, 10):\n",
    "    if round_num < 9:\n",
    "        continue\n",
    "    test_output = torch.load(\"/home/datamake117/data/haris/DL/\" + main_folder_name + \"/test_output_\" + str(round_num) + \".pt\")\n",
    "    test_output_list.append(test_output)\n",
    "test_output = torch.cat(test_output_list)\n",
    "test_output = test_output.cpu()\n",
    "date_pid_list = []\n",
    "for round_num in range(2, 10):\n",
    "    if round_num < 9:\n",
    "        continue\n",
    "    date_pid = torch.load(\"/home/datamake117/data/haris/DL/\" + main_folder_name + \"/test_date_pid_\" + str(round_num) + \".pt\", weights_only=False)\n",
    "    date_pid_list.append(date_pid)\n",
    "total_date_pid = np.concatenate(date_pid_list, axis=0)\n",
    "total_date_pid_test = total_date_pid\n",
    "grading_factor = pd.DataFrame(index=np.unique(total_date_pid_test[:, 0]), columns=np.unique(total_date_pid_test[:, 1]))\n",
    "test_output_list = test_output.tolist()\n",
    "for i in range(len(total_date_pid_test)):\n",
    "    grading_factor.loc[total_date_pid_test[i][0], total_date_pid_test[i][1]] = test_output_list[i]\n",
    "grading_factor = pd.concat([grading_factor], axis=0)\n",
    "grading_factor.to_feather(\"/home/datamake117/data/haris/DL/\" + main_folder_name + \"/单次_KFold_0_period9.fea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000001</th>\n",
       "      <th>000002</th>\n",
       "      <th>000004</th>\n",
       "      <th>000005</th>\n",
       "      <th>000006</th>\n",
       "      <th>000007</th>\n",
       "      <th>000008</th>\n",
       "      <th>000009</th>\n",
       "      <th>000010</th>\n",
       "      <th>000011</th>\n",
       "      <th>...</th>\n",
       "      <th>688787</th>\n",
       "      <th>688788</th>\n",
       "      <th>688789</th>\n",
       "      <th>688793</th>\n",
       "      <th>688798</th>\n",
       "      <th>688799</th>\n",
       "      <th>688800</th>\n",
       "      <th>688819</th>\n",
       "      <th>688981</th>\n",
       "      <th>689009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20250102</th>\n",
       "      <td>-0.904052</td>\n",
       "      <td>0.089991</td>\n",
       "      <td>2.092479</td>\n",
       "      <td>0.134961</td>\n",
       "      <td>-0.049566</td>\n",
       "      <td>0.228958</td>\n",
       "      <td>0.056152</td>\n",
       "      <td>-0.138607</td>\n",
       "      <td>2.041149</td>\n",
       "      <td>0.288532</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081127</td>\n",
       "      <td>-0.676493</td>\n",
       "      <td>-0.664498</td>\n",
       "      <td>0.814576</td>\n",
       "      <td>0.244681</td>\n",
       "      <td>-0.675281</td>\n",
       "      <td>-0.749205</td>\n",
       "      <td>-0.114281</td>\n",
       "      <td>0.767347</td>\n",
       "      <td>-0.409309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250103</th>\n",
       "      <td>-0.489799</td>\n",
       "      <td>-0.567948</td>\n",
       "      <td>0.84887</td>\n",
       "      <td>0.134961</td>\n",
       "      <td>-0.21165</td>\n",
       "      <td>-1.765285</td>\n",
       "      <td>-0.096718</td>\n",
       "      <td>-0.0477</td>\n",
       "      <td>0.573878</td>\n",
       "      <td>1.203264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.781348</td>\n",
       "      <td>-0.170623</td>\n",
       "      <td>-0.691842</td>\n",
       "      <td>0.263831</td>\n",
       "      <td>-0.373882</td>\n",
       "      <td>-0.664265</td>\n",
       "      <td>-1.056103</td>\n",
       "      <td>-0.211608</td>\n",
       "      <td>-0.904616</td>\n",
       "      <td>-1.180849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250106</th>\n",
       "      <td>-0.338798</td>\n",
       "      <td>0.357211</td>\n",
       "      <td>3.631</td>\n",
       "      <td>0.134961</td>\n",
       "      <td>0.347434</td>\n",
       "      <td>0.238773</td>\n",
       "      <td>2.492587</td>\n",
       "      <td>0.481124</td>\n",
       "      <td>0.95551</td>\n",
       "      <td>1.157385</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.458895</td>\n",
       "      <td>0.621874</td>\n",
       "      <td>-0.534907</td>\n",
       "      <td>0.407573</td>\n",
       "      <td>0.126543</td>\n",
       "      <td>-0.479251</td>\n",
       "      <td>-0.397901</td>\n",
       "      <td>-0.459894</td>\n",
       "      <td>-0.080859</td>\n",
       "      <td>-0.578569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250107</th>\n",
       "      <td>-0.760277</td>\n",
       "      <td>-0.044117</td>\n",
       "      <td>1.462992</td>\n",
       "      <td>0.134961</td>\n",
       "      <td>-0.204369</td>\n",
       "      <td>-0.433124</td>\n",
       "      <td>0.411026</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.214352</td>\n",
       "      <td>0.883486</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.096836</td>\n",
       "      <td>-0.432892</td>\n",
       "      <td>-0.856083</td>\n",
       "      <td>-1.303184</td>\n",
       "      <td>0.171071</td>\n",
       "      <td>-0.64426</td>\n",
       "      <td>-1.094856</td>\n",
       "      <td>-0.236792</td>\n",
       "      <td>-0.617414</td>\n",
       "      <td>-1.019745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250108</th>\n",
       "      <td>-0.44256</td>\n",
       "      <td>0.029288</td>\n",
       "      <td>2.236714</td>\n",
       "      <td>0.134961</td>\n",
       "      <td>0.399007</td>\n",
       "      <td>-0.743933</td>\n",
       "      <td>1.071878</td>\n",
       "      <td>-0.258853</td>\n",
       "      <td>0.665936</td>\n",
       "      <td>0.528503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.976907</td>\n",
       "      <td>-0.773776</td>\n",
       "      <td>-0.696915</td>\n",
       "      <td>-0.606397</td>\n",
       "      <td>-1.063534</td>\n",
       "      <td>-0.268603</td>\n",
       "      <td>-0.358294</td>\n",
       "      <td>-0.681916</td>\n",
       "      <td>-2.059001</td>\n",
       "      <td>-1.08844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250109</th>\n",
       "      <td>-0.490571</td>\n",
       "      <td>0.188457</td>\n",
       "      <td>1.200161</td>\n",
       "      <td>0.188636</td>\n",
       "      <td>-0.18551</td>\n",
       "      <td>-0.293889</td>\n",
       "      <td>0.660982</td>\n",
       "      <td>0.789255</td>\n",
       "      <td>0.621765</td>\n",
       "      <td>0.152752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123769</td>\n",
       "      <td>0.083743</td>\n",
       "      <td>0.230231</td>\n",
       "      <td>-0.090631</td>\n",
       "      <td>-1.008725</td>\n",
       "      <td>0.108681</td>\n",
       "      <td>-3.517992</td>\n",
       "      <td>-0.529102</td>\n",
       "      <td>-0.190433</td>\n",
       "      <td>-0.634504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250110</th>\n",
       "      <td>-0.365009</td>\n",
       "      <td>-0.288838</td>\n",
       "      <td>0.497971</td>\n",
       "      <td>0.188636</td>\n",
       "      <td>-0.349784</td>\n",
       "      <td>-1.189376</td>\n",
       "      <td>0.65501</td>\n",
       "      <td>0.214464</td>\n",
       "      <td>0.092414</td>\n",
       "      <td>-0.083319</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.702905</td>\n",
       "      <td>-0.652699</td>\n",
       "      <td>-0.626355</td>\n",
       "      <td>0.04872</td>\n",
       "      <td>-0.291953</td>\n",
       "      <td>-0.627871</td>\n",
       "      <td>-2.061929</td>\n",
       "      <td>-0.068576</td>\n",
       "      <td>-0.497818</td>\n",
       "      <td>-1.076716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250113</th>\n",
       "      <td>0.213688</td>\n",
       "      <td>0.71962</td>\n",
       "      <td>1.830424</td>\n",
       "      <td>0.188636</td>\n",
       "      <td>0.894913</td>\n",
       "      <td>1.023854</td>\n",
       "      <td>0.8168</td>\n",
       "      <td>1.040991</td>\n",
       "      <td>0.569226</td>\n",
       "      <td>0.794444</td>\n",
       "      <td>...</td>\n",
       "      <td>1.225399</td>\n",
       "      <td>0.456504</td>\n",
       "      <td>-0.436244</td>\n",
       "      <td>1.011631</td>\n",
       "      <td>-0.20339</td>\n",
       "      <td>-0.037062</td>\n",
       "      <td>-0.226455</td>\n",
       "      <td>0.40153</td>\n",
       "      <td>0.463044</td>\n",
       "      <td>0.238483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250114</th>\n",
       "      <td>-1.101374</td>\n",
       "      <td>-0.035389</td>\n",
       "      <td>-0.252416</td>\n",
       "      <td>0.188636</td>\n",
       "      <td>-0.312758</td>\n",
       "      <td>-0.402604</td>\n",
       "      <td>0.313011</td>\n",
       "      <td>-0.418676</td>\n",
       "      <td>-1.044771</td>\n",
       "      <td>-0.479961</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.416077</td>\n",
       "      <td>-0.278662</td>\n",
       "      <td>-0.62209</td>\n",
       "      <td>0.330344</td>\n",
       "      <td>-0.086687</td>\n",
       "      <td>-0.571023</td>\n",
       "      <td>-2.081038</td>\n",
       "      <td>-1.108473</td>\n",
       "      <td>-2.719101</td>\n",
       "      <td>-0.428268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250115</th>\n",
       "      <td>-0.144947</td>\n",
       "      <td>-0.122055</td>\n",
       "      <td>0.161258</td>\n",
       "      <td>0.188636</td>\n",
       "      <td>-0.272139</td>\n",
       "      <td>-0.367372</td>\n",
       "      <td>-0.054225</td>\n",
       "      <td>-0.145526</td>\n",
       "      <td>-0.761952</td>\n",
       "      <td>-0.006172</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.662644</td>\n",
       "      <td>0.398402</td>\n",
       "      <td>-1.194339</td>\n",
       "      <td>-0.457727</td>\n",
       "      <td>-0.851418</td>\n",
       "      <td>-0.517234</td>\n",
       "      <td>-2.329559</td>\n",
       "      <td>-0.902747</td>\n",
       "      <td>-1.388252</td>\n",
       "      <td>-0.483791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250116</th>\n",
       "      <td>-0.83374</td>\n",
       "      <td>-0.205608</td>\n",
       "      <td>-0.014233</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>-0.511956</td>\n",
       "      <td>-1.219377</td>\n",
       "      <td>0.099685</td>\n",
       "      <td>-0.197575</td>\n",
       "      <td>-1.258955</td>\n",
       "      <td>-0.55858</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.506233</td>\n",
       "      <td>-1.221387</td>\n",
       "      <td>-1.148745</td>\n",
       "      <td>0.352039</td>\n",
       "      <td>-0.413072</td>\n",
       "      <td>-0.513386</td>\n",
       "      <td>-0.475044</td>\n",
       "      <td>-0.473702</td>\n",
       "      <td>-0.972741</td>\n",
       "      <td>-0.151894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250117</th>\n",
       "      <td>0.71659</td>\n",
       "      <td>1.378049</td>\n",
       "      <td>1.481304</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>0.110378</td>\n",
       "      <td>0.103297</td>\n",
       "      <td>2.666393</td>\n",
       "      <td>0.586274</td>\n",
       "      <td>1.210542</td>\n",
       "      <td>0.979752</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485306</td>\n",
       "      <td>0.040027</td>\n",
       "      <td>-0.23091</td>\n",
       "      <td>0.22723</td>\n",
       "      <td>0.173033</td>\n",
       "      <td>-0.165955</td>\n",
       "      <td>0.27819</td>\n",
       "      <td>0.062366</td>\n",
       "      <td>1.491873</td>\n",
       "      <td>0.396571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250120</th>\n",
       "      <td>-0.636079</td>\n",
       "      <td>0.979383</td>\n",
       "      <td>-0.426423</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>-0.736014</td>\n",
       "      <td>-0.245956</td>\n",
       "      <td>0.139384</td>\n",
       "      <td>-0.263891</td>\n",
       "      <td>-0.398975</td>\n",
       "      <td>0.194464</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.154701</td>\n",
       "      <td>0.03157</td>\n",
       "      <td>-1.281569</td>\n",
       "      <td>-0.621922</td>\n",
       "      <td>-0.699626</td>\n",
       "      <td>-0.392538</td>\n",
       "      <td>-0.841374</td>\n",
       "      <td>-0.568553</td>\n",
       "      <td>-1.620166</td>\n",
       "      <td>-0.411724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250121</th>\n",
       "      <td>-0.437636</td>\n",
       "      <td>0.496434</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>0.289929</td>\n",
       "      <td>-0.445273</td>\n",
       "      <td>1.301967</td>\n",
       "      <td>-0.467205</td>\n",
       "      <td>0.436877</td>\n",
       "      <td>0.114003</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.02098</td>\n",
       "      <td>-0.138552</td>\n",
       "      <td>-0.644838</td>\n",
       "      <td>-0.641193</td>\n",
       "      <td>-0.286198</td>\n",
       "      <td>-0.237758</td>\n",
       "      <td>0.169772</td>\n",
       "      <td>-1.22119</td>\n",
       "      <td>-1.463238</td>\n",
       "      <td>-0.702478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250122</th>\n",
       "      <td>0.413085</td>\n",
       "      <td>2.319715</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>0.481172</td>\n",
       "      <td>2.788935</td>\n",
       "      <td>0.748651</td>\n",
       "      <td>2.171725</td>\n",
       "      <td>0.536698</td>\n",
       "      <td>1.01853</td>\n",
       "      <td>0.096151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047342</td>\n",
       "      <td>-0.230831</td>\n",
       "      <td>-0.136118</td>\n",
       "      <td>0.962964</td>\n",
       "      <td>0.028411</td>\n",
       "      <td>0.029583</td>\n",
       "      <td>0.294853</td>\n",
       "      <td>0.216354</td>\n",
       "      <td>0.484573</td>\n",
       "      <td>0.322657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250123</th>\n",
       "      <td>-1.187324</td>\n",
       "      <td>-0.600821</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>-0.380372</td>\n",
       "      <td>-0.421434</td>\n",
       "      <td>0.122731</td>\n",
       "      <td>-0.205242</td>\n",
       "      <td>-0.294128</td>\n",
       "      <td>-0.054435</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.55001</td>\n",
       "      <td>-0.521434</td>\n",
       "      <td>-1.101248</td>\n",
       "      <td>0.517351</td>\n",
       "      <td>-0.519873</td>\n",
       "      <td>-0.72536</td>\n",
       "      <td>-1.133254</td>\n",
       "      <td>-0.702542</td>\n",
       "      <td>-1.177685</td>\n",
       "      <td>-1.782798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250124</th>\n",
       "      <td>0.354776</td>\n",
       "      <td>1.345425</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>1.513576</td>\n",
       "      <td>0.291408</td>\n",
       "      <td>0.633418</td>\n",
       "      <td>0.408217</td>\n",
       "      <td>1.14671</td>\n",
       "      <td>1.553364</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268643</td>\n",
       "      <td>0.168638</td>\n",
       "      <td>-0.487115</td>\n",
       "      <td>0.648739</td>\n",
       "      <td>-0.840442</td>\n",
       "      <td>-0.252846</td>\n",
       "      <td>1.047189</td>\n",
       "      <td>0.231064</td>\n",
       "      <td>0.945011</td>\n",
       "      <td>0.323267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250127</th>\n",
       "      <td>0.38319</td>\n",
       "      <td>0.060693</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.365421</td>\n",
       "      <td>0.140999</td>\n",
       "      <td>0.924113</td>\n",
       "      <td>0.587986</td>\n",
       "      <td>0.22245</td>\n",
       "      <td>0.662811</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.231331</td>\n",
       "      <td>0.059523</td>\n",
       "      <td>-0.682082</td>\n",
       "      <td>0.176004</td>\n",
       "      <td>0.035276</td>\n",
       "      <td>-0.657422</td>\n",
       "      <td>-0.408648</td>\n",
       "      <td>-0.599464</td>\n",
       "      <td>-0.724188</td>\n",
       "      <td>-3.532338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250205</th>\n",
       "      <td>-0.665536</td>\n",
       "      <td>0.610094</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.339387</td>\n",
       "      <td>-0.617755</td>\n",
       "      <td>-0.324231</td>\n",
       "      <td>-0.020597</td>\n",
       "      <td>-1.340413</td>\n",
       "      <td>0.326736</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.78613</td>\n",
       "      <td>0.153687</td>\n",
       "      <td>-1.04286</td>\n",
       "      <td>0.093426</td>\n",
       "      <td>-0.427195</td>\n",
       "      <td>-0.439719</td>\n",
       "      <td>-0.081822</td>\n",
       "      <td>-0.51589</td>\n",
       "      <td>-1.562917</td>\n",
       "      <td>-1.534113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250206</th>\n",
       "      <td>0.194748</td>\n",
       "      <td>1.356969</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.509987</td>\n",
       "      <td>0.494476</td>\n",
       "      <td>0.426212</td>\n",
       "      <td>1.621534</td>\n",
       "      <td>0.528174</td>\n",
       "      <td>0.350154</td>\n",
       "      <td>0.727002</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.709728</td>\n",
       "      <td>-0.061796</td>\n",
       "      <td>0.948837</td>\n",
       "      <td>0.533935</td>\n",
       "      <td>0.043839</td>\n",
       "      <td>-0.24486</td>\n",
       "      <td>-0.067107</td>\n",
       "      <td>0.212994</td>\n",
       "      <td>1.416902</td>\n",
       "      <td>0.138353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250207</th>\n",
       "      <td>-0.462197</td>\n",
       "      <td>0.618074</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.993662</td>\n",
       "      <td>-0.210399</td>\n",
       "      <td>-0.241515</td>\n",
       "      <td>0.145889</td>\n",
       "      <td>1.327986</td>\n",
       "      <td>0.68834</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.04485</td>\n",
       "      <td>-0.011086</td>\n",
       "      <td>-0.295617</td>\n",
       "      <td>0.157252</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.734983</td>\n",
       "      <td>-0.376432</td>\n",
       "      <td>-0.36213</td>\n",
       "      <td>-1.040764</td>\n",
       "      <td>-0.084758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250210</th>\n",
       "      <td>-0.228507</td>\n",
       "      <td>1.436073</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>1.743286</td>\n",
       "      <td>0.241101</td>\n",
       "      <td>-0.076097</td>\n",
       "      <td>-0.267054</td>\n",
       "      <td>0.487968</td>\n",
       "      <td>0.44234</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.313093</td>\n",
       "      <td>-0.009591</td>\n",
       "      <td>0.051791</td>\n",
       "      <td>0.370303</td>\n",
       "      <td>-0.540463</td>\n",
       "      <td>0.050606</td>\n",
       "      <td>-0.28923</td>\n",
       "      <td>-0.14355</td>\n",
       "      <td>-1.042885</td>\n",
       "      <td>-0.01569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250211</th>\n",
       "      <td>0.171297</td>\n",
       "      <td>1.038285</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.81585</td>\n",
       "      <td>0.657488</td>\n",
       "      <td>0.050522</td>\n",
       "      <td>-0.114854</td>\n",
       "      <td>0.268683</td>\n",
       "      <td>0.23661</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.967903</td>\n",
       "      <td>0.237451</td>\n",
       "      <td>-0.563694</td>\n",
       "      <td>-1.343087</td>\n",
       "      <td>-0.205119</td>\n",
       "      <td>-0.265016</td>\n",
       "      <td>-0.232323</td>\n",
       "      <td>0.076955</td>\n",
       "      <td>-1.104271</td>\n",
       "      <td>0.686453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250212</th>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.834172</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>1.130847</td>\n",
       "      <td>1.022984</td>\n",
       "      <td>0.023015</td>\n",
       "      <td>-0.015548</td>\n",
       "      <td>0.844926</td>\n",
       "      <td>0.875878</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.262587</td>\n",
       "      <td>-0.06065</td>\n",
       "      <td>-0.267067</td>\n",
       "      <td>-0.207975</td>\n",
       "      <td>0.578494</td>\n",
       "      <td>-0.190099</td>\n",
       "      <td>-1.019712</td>\n",
       "      <td>0.272052</td>\n",
       "      <td>0.113853</td>\n",
       "      <td>-0.285097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250213</th>\n",
       "      <td>-0.277921</td>\n",
       "      <td>0.786492</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.479445</td>\n",
       "      <td>0.819894</td>\n",
       "      <td>0.631549</td>\n",
       "      <td>0.919848</td>\n",
       "      <td>0.560483</td>\n",
       "      <td>0.842274</td>\n",
       "      <td>1.17639</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.578134</td>\n",
       "      <td>0.394233</td>\n",
       "      <td>-1.138999</td>\n",
       "      <td>0.188936</td>\n",
       "      <td>-0.318727</td>\n",
       "      <td>0.186141</td>\n",
       "      <td>-1.1158</td>\n",
       "      <td>-0.311047</td>\n",
       "      <td>-1.272015</td>\n",
       "      <td>0.227165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250214</th>\n",
       "      <td>-0.082182</td>\n",
       "      <td>1.289042</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.713305</td>\n",
       "      <td>0.04205</td>\n",
       "      <td>0.774715</td>\n",
       "      <td>-0.229435</td>\n",
       "      <td>-0.722213</td>\n",
       "      <td>0.118425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.633342</td>\n",
       "      <td>0.559451</td>\n",
       "      <td>-0.096406</td>\n",
       "      <td>0.072936</td>\n",
       "      <td>-0.152379</td>\n",
       "      <td>0.471758</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>-0.255451</td>\n",
       "      <td>0.47977</td>\n",
       "      <td>0.049429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250217</th>\n",
       "      <td>-0.211188</td>\n",
       "      <td>0.862637</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.128421</td>\n",
       "      <td>-0.092509</td>\n",
       "      <td>-0.133109</td>\n",
       "      <td>-0.112063</td>\n",
       "      <td>-0.041085</td>\n",
       "      <td>0.542998</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.033899</td>\n",
       "      <td>-0.179534</td>\n",
       "      <td>-0.732763</td>\n",
       "      <td>0.539017</td>\n",
       "      <td>-0.309814</td>\n",
       "      <td>-0.279501</td>\n",
       "      <td>-0.183784</td>\n",
       "      <td>-0.176494</td>\n",
       "      <td>-1.348357</td>\n",
       "      <td>-1.520508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250218</th>\n",
       "      <td>0.177768</td>\n",
       "      <td>1.109487</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.641296</td>\n",
       "      <td>1.116441</td>\n",
       "      <td>0.094352</td>\n",
       "      <td>0.336046</td>\n",
       "      <td>0.065078</td>\n",
       "      <td>0.431888</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.807196</td>\n",
       "      <td>-2.199323</td>\n",
       "      <td>0.025144</td>\n",
       "      <td>0.17397</td>\n",
       "      <td>-0.213371</td>\n",
       "      <td>0.180576</td>\n",
       "      <td>0.09672</td>\n",
       "      <td>-0.627554</td>\n",
       "      <td>-1.263454</td>\n",
       "      <td>-0.030244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250219</th>\n",
       "      <td>0.497883</td>\n",
       "      <td>0.601546</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.795135</td>\n",
       "      <td>0.969581</td>\n",
       "      <td>0.99955</td>\n",
       "      <td>0.328108</td>\n",
       "      <td>1.114255</td>\n",
       "      <td>1.383636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.849816</td>\n",
       "      <td>0.328449</td>\n",
       "      <td>-0.586084</td>\n",
       "      <td>-0.187158</td>\n",
       "      <td>-0.067611</td>\n",
       "      <td>-0.083202</td>\n",
       "      <td>0.080432</td>\n",
       "      <td>-0.840097</td>\n",
       "      <td>-0.532761</td>\n",
       "      <td>-0.511964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250220</th>\n",
       "      <td>0.11167</td>\n",
       "      <td>-0.067062</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.260918</td>\n",
       "      <td>0.638411</td>\n",
       "      <td>0.30996</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>0.227155</td>\n",
       "      <td>0.172854</td>\n",
       "      <td>0.43558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430836</td>\n",
       "      <td>-0.872055</td>\n",
       "      <td>-0.199961</td>\n",
       "      <td>-0.149572</td>\n",
       "      <td>-0.270188</td>\n",
       "      <td>-0.083423</td>\n",
       "      <td>-0.042412</td>\n",
       "      <td>0.06029</td>\n",
       "      <td>-0.469854</td>\n",
       "      <td>-1.351192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250221</th>\n",
       "      <td>0.212058</td>\n",
       "      <td>0.480046</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>-1.15476</td>\n",
       "      <td>0.569544</td>\n",
       "      <td>0.946206</td>\n",
       "      <td>0.050251</td>\n",
       "      <td>0.493398</td>\n",
       "      <td>0.374904</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.878866</td>\n",
       "      <td>-1.687277</td>\n",
       "      <td>-0.508964</td>\n",
       "      <td>-0.029213</td>\n",
       "      <td>-0.702672</td>\n",
       "      <td>-0.115967</td>\n",
       "      <td>-0.00501</td>\n",
       "      <td>-0.301042</td>\n",
       "      <td>-0.575859</td>\n",
       "      <td>-1.052428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250224</th>\n",
       "      <td>0.092816</td>\n",
       "      <td>0.765087</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>1.112577</td>\n",
       "      <td>0.963884</td>\n",
       "      <td>-0.436032</td>\n",
       "      <td>0.136437</td>\n",
       "      <td>0.215957</td>\n",
       "      <td>0.808383</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.486165</td>\n",
       "      <td>-0.401103</td>\n",
       "      <td>-0.004293</td>\n",
       "      <td>0.032972</td>\n",
       "      <td>0.07202</td>\n",
       "      <td>0.187267</td>\n",
       "      <td>0.27004</td>\n",
       "      <td>-0.823024</td>\n",
       "      <td>0.034948</td>\n",
       "      <td>-0.367174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250225</th>\n",
       "      <td>0.356005</td>\n",
       "      <td>1.250742</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>1.597188</td>\n",
       "      <td>1.017963</td>\n",
       "      <td>0.712668</td>\n",
       "      <td>0.688047</td>\n",
       "      <td>1.893106</td>\n",
       "      <td>1.509907</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206328</td>\n",
       "      <td>-0.638099</td>\n",
       "      <td>0.411152</td>\n",
       "      <td>0.708137</td>\n",
       "      <td>0.299435</td>\n",
       "      <td>0.035955</td>\n",
       "      <td>0.680142</td>\n",
       "      <td>0.238159</td>\n",
       "      <td>-0.280613</td>\n",
       "      <td>0.936119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250226</th>\n",
       "      <td>0.313859</td>\n",
       "      <td>0.187905</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.670144</td>\n",
       "      <td>0.272139</td>\n",
       "      <td>-0.102237</td>\n",
       "      <td>0.365359</td>\n",
       "      <td>-0.64588</td>\n",
       "      <td>0.699616</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.046828</td>\n",
       "      <td>-0.612404</td>\n",
       "      <td>-0.748629</td>\n",
       "      <td>0.173367</td>\n",
       "      <td>-0.310093</td>\n",
       "      <td>-0.401008</td>\n",
       "      <td>-0.539452</td>\n",
       "      <td>0.369914</td>\n",
       "      <td>-2.444743</td>\n",
       "      <td>-0.708457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250227</th>\n",
       "      <td>-0.352533</td>\n",
       "      <td>0.247165</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.253018</td>\n",
       "      <td>0.425912</td>\n",
       "      <td>0.847922</td>\n",
       "      <td>0.057561</td>\n",
       "      <td>0.256181</td>\n",
       "      <td>0.33191</td>\n",
       "      <td>0.529335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.897373</td>\n",
       "      <td>-0.330388</td>\n",
       "      <td>-0.205192</td>\n",
       "      <td>0.234017</td>\n",
       "      <td>-2.344728</td>\n",
       "      <td>-0.272062</td>\n",
       "      <td>-1.716527</td>\n",
       "      <td>-0.177447</td>\n",
       "      <td>-0.467317</td>\n",
       "      <td>-0.889173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250228</th>\n",
       "      <td>0.612892</td>\n",
       "      <td>0.787548</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>2.919862</td>\n",
       "      <td>0.864507</td>\n",
       "      <td>1.781937</td>\n",
       "      <td>0.752383</td>\n",
       "      <td>0.424391</td>\n",
       "      <td>1.034583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334181</td>\n",
       "      <td>0.082271</td>\n",
       "      <td>0.178394</td>\n",
       "      <td>1.056943</td>\n",
       "      <td>-0.316552</td>\n",
       "      <td>0.280214</td>\n",
       "      <td>-0.109341</td>\n",
       "      <td>-0.0798</td>\n",
       "      <td>0.024783</td>\n",
       "      <td>0.029961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250303</th>\n",
       "      <td>-0.190661</td>\n",
       "      <td>1.334926</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.104649</td>\n",
       "      <td>0.395411</td>\n",
       "      <td>1.600276</td>\n",
       "      <td>-0.328663</td>\n",
       "      <td>0.052872</td>\n",
       "      <td>0.375762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118165</td>\n",
       "      <td>-1.234503</td>\n",
       "      <td>-1.28424</td>\n",
       "      <td>-0.070713</td>\n",
       "      <td>0.190068</td>\n",
       "      <td>-0.707869</td>\n",
       "      <td>-0.740117</td>\n",
       "      <td>0.391974</td>\n",
       "      <td>-0.27074</td>\n",
       "      <td>-0.139644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250304</th>\n",
       "      <td>0.449972</td>\n",
       "      <td>0.927822</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>1.180857</td>\n",
       "      <td>0.591738</td>\n",
       "      <td>0.463992</td>\n",
       "      <td>0.461512</td>\n",
       "      <td>0.831143</td>\n",
       "      <td>0.685321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350758</td>\n",
       "      <td>-0.01777</td>\n",
       "      <td>-0.569366</td>\n",
       "      <td>0.532062</td>\n",
       "      <td>0.270578</td>\n",
       "      <td>-0.135028</td>\n",
       "      <td>0.630126</td>\n",
       "      <td>0.749328</td>\n",
       "      <td>0.075274</td>\n",
       "      <td>0.015627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250305</th>\n",
       "      <td>-0.256351</td>\n",
       "      <td>0.237364</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>-0.262909</td>\n",
       "      <td>0.156843</td>\n",
       "      <td>-0.625579</td>\n",
       "      <td>-0.112959</td>\n",
       "      <td>-0.508872</td>\n",
       "      <td>-0.378252</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.10093</td>\n",
       "      <td>-0.795031</td>\n",
       "      <td>-1.261865</td>\n",
       "      <td>0.366697</td>\n",
       "      <td>-1.559003</td>\n",
       "      <td>-0.33626</td>\n",
       "      <td>-0.479428</td>\n",
       "      <td>-0.761519</td>\n",
       "      <td>-0.969183</td>\n",
       "      <td>-0.556107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250306</th>\n",
       "      <td>-0.130246</td>\n",
       "      <td>-0.458354</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>0.307781</td>\n",
       "      <td>-0.81974</td>\n",
       "      <td>-0.434596</td>\n",
       "      <td>0.356716</td>\n",
       "      <td>-0.864816</td>\n",
       "      <td>0.181316</td>\n",
       "      <td>-0.196416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.811404</td>\n",
       "      <td>-1.065266</td>\n",
       "      <td>-0.456791</td>\n",
       "      <td>-1.516444</td>\n",
       "      <td>-0.836757</td>\n",
       "      <td>-0.576073</td>\n",
       "      <td>-0.520103</td>\n",
       "      <td>-0.299478</td>\n",
       "      <td>-1.887144</td>\n",
       "      <td>-1.445656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250307</th>\n",
       "      <td>0.59883</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>-0.040834</td>\n",
       "      <td>0.518876</td>\n",
       "      <td>0.393081</td>\n",
       "      <td>0.240779</td>\n",
       "      <td>-0.063037</td>\n",
       "      <td>0.13689</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285011</td>\n",
       "      <td>-0.33447</td>\n",
       "      <td>-0.73926</td>\n",
       "      <td>-0.034199</td>\n",
       "      <td>-0.692943</td>\n",
       "      <td>0.068347</td>\n",
       "      <td>-0.183223</td>\n",
       "      <td>0.08561</td>\n",
       "      <td>-0.45157</td>\n",
       "      <td>0.092991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250310</th>\n",
       "      <td>0.45205</td>\n",
       "      <td>0.104582</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>-0.707551</td>\n",
       "      <td>0.022267</td>\n",
       "      <td>-0.901914</td>\n",
       "      <td>0.153245</td>\n",
       "      <td>0.296254</td>\n",
       "      <td>0.152991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251678</td>\n",
       "      <td>-2.060071</td>\n",
       "      <td>-0.536677</td>\n",
       "      <td>-0.859122</td>\n",
       "      <td>0.122379</td>\n",
       "      <td>-0.568772</td>\n",
       "      <td>-0.437408</td>\n",
       "      <td>-0.426997</td>\n",
       "      <td>0.269505</td>\n",
       "      <td>-1.050106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250311</th>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.762582</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>-2.519167</td>\n",
       "      <td>0.755088</td>\n",
       "      <td>0.411208</td>\n",
       "      <td>0.860003</td>\n",
       "      <td>0.746597</td>\n",
       "      <td>0.875219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12352</td>\n",
       "      <td>0.203004</td>\n",
       "      <td>0.759749</td>\n",
       "      <td>0.379899</td>\n",
       "      <td>0.278806</td>\n",
       "      <td>0.031479</td>\n",
       "      <td>1.076025</td>\n",
       "      <td>0.811853</td>\n",
       "      <td>0.130277</td>\n",
       "      <td>0.940331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250312</th>\n",
       "      <td>-0.808633</td>\n",
       "      <td>-0.483864</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.251129</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>-0.258457</td>\n",
       "      <td>-0.415855</td>\n",
       "      <td>-0.310729</td>\n",
       "      <td>0.244227</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.439826</td>\n",
       "      <td>-2.527123</td>\n",
       "      <td>-0.692771</td>\n",
       "      <td>-0.889919</td>\n",
       "      <td>-0.553083</td>\n",
       "      <td>-0.315473</td>\n",
       "      <td>-0.626193</td>\n",
       "      <td>-0.412816</td>\n",
       "      <td>-0.919056</td>\n",
       "      <td>-0.314424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250313</th>\n",
       "      <td>0.181348</td>\n",
       "      <td>0.457114</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.062277</td>\n",
       "      <td>-0.389263</td>\n",
       "      <td>0.516293</td>\n",
       "      <td>0.338254</td>\n",
       "      <td>0.83128</td>\n",
       "      <td>0.329332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.881482</td>\n",
       "      <td>-0.332239</td>\n",
       "      <td>-0.075963</td>\n",
       "      <td>0.81813</td>\n",
       "      <td>-0.150763</td>\n",
       "      <td>-0.18858</td>\n",
       "      <td>-0.647618</td>\n",
       "      <td>0.26041</td>\n",
       "      <td>0.217892</td>\n",
       "      <td>-0.626144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250314</th>\n",
       "      <td>0.758573</td>\n",
       "      <td>0.03519</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>-0.019026</td>\n",
       "      <td>0.199495</td>\n",
       "      <td>0.632333</td>\n",
       "      <td>0.198539</td>\n",
       "      <td>0.593357</td>\n",
       "      <td>0.37929</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.250091</td>\n",
       "      <td>-1.127409</td>\n",
       "      <td>-0.398753</td>\n",
       "      <td>-0.289377</td>\n",
       "      <td>-0.301693</td>\n",
       "      <td>0.20669</td>\n",
       "      <td>-1.747086</td>\n",
       "      <td>-0.013087</td>\n",
       "      <td>0.163502</td>\n",
       "      <td>-0.395579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250317</th>\n",
       "      <td>-0.689106</td>\n",
       "      <td>-0.133325</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>0.062284</td>\n",
       "      <td>0.322409</td>\n",
       "      <td>1.009955</td>\n",
       "      <td>0.309609</td>\n",
       "      <td>1.018921</td>\n",
       "      <td>0.278765</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.380948</td>\n",
       "      <td>-1.999367</td>\n",
       "      <td>-0.443132</td>\n",
       "      <td>0.223071</td>\n",
       "      <td>0.276311</td>\n",
       "      <td>-0.955541</td>\n",
       "      <td>-0.949314</td>\n",
       "      <td>0.163168</td>\n",
       "      <td>-0.857937</td>\n",
       "      <td>-2.264636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250318</th>\n",
       "      <td>-0.25353</td>\n",
       "      <td>0.276067</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>-0.259088</td>\n",
       "      <td>-0.885714</td>\n",
       "      <td>-0.501194</td>\n",
       "      <td>-0.441942</td>\n",
       "      <td>0.907131</td>\n",
       "      <td>0.027689</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.895608</td>\n",
       "      <td>-3.214111</td>\n",
       "      <td>-0.131925</td>\n",
       "      <td>-0.107304</td>\n",
       "      <td>-0.759922</td>\n",
       "      <td>-0.023932</td>\n",
       "      <td>-0.731775</td>\n",
       "      <td>-0.45467</td>\n",
       "      <td>-0.972818</td>\n",
       "      <td>-0.317917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250319</th>\n",
       "      <td>-0.236468</td>\n",
       "      <td>0.26552</td>\n",
       "      <td>0.264722</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>0.422298</td>\n",
       "      <td>0.304992</td>\n",
       "      <td>0.643591</td>\n",
       "      <td>0.493778</td>\n",
       "      <td>0.975399</td>\n",
       "      <td>0.827988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431055</td>\n",
       "      <td>-1.909187</td>\n",
       "      <td>0.394311</td>\n",
       "      <td>0.104988</td>\n",
       "      <td>0.17699</td>\n",
       "      <td>0.189081</td>\n",
       "      <td>-0.239644</td>\n",
       "      <td>0.60685</td>\n",
       "      <td>0.049232</td>\n",
       "      <td>1.016428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250320</th>\n",
       "      <td>-0.242419</td>\n",
       "      <td>0.221612</td>\n",
       "      <td>0.901015</td>\n",
       "      <td>0.364875</td>\n",
       "      <td>0.253279</td>\n",
       "      <td>0.455646</td>\n",
       "      <td>0.590378</td>\n",
       "      <td>0.600963</td>\n",
       "      <td>1.313962</td>\n",
       "      <td>0.876602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.745062</td>\n",
       "      <td>-0.564776</td>\n",
       "      <td>-0.515208</td>\n",
       "      <td>-0.15012</td>\n",
       "      <td>0.237059</td>\n",
       "      <td>0.011149</td>\n",
       "      <td>-0.074946</td>\n",
       "      <td>0.383264</td>\n",
       "      <td>-0.384802</td>\n",
       "      <td>-0.48722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250321</th>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.528542</td>\n",
       "      <td>1.517993</td>\n",
       "      <td>0.10211</td>\n",
       "      <td>0.219791</td>\n",
       "      <td>-0.167149</td>\n",
       "      <td>-0.150813</td>\n",
       "      <td>0.188086</td>\n",
       "      <td>0.298946</td>\n",
       "      <td>0.489481</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.208437</td>\n",
       "      <td>-0.632284</td>\n",
       "      <td>-0.103244</td>\n",
       "      <td>0.510846</td>\n",
       "      <td>-0.566127</td>\n",
       "      <td>-0.499798</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>-0.120563</td>\n",
       "      <td>0.01431</td>\n",
       "      <td>-1.272805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250324</th>\n",
       "      <td>-0.231402</td>\n",
       "      <td>-0.403474</td>\n",
       "      <td>0.279134</td>\n",
       "      <td>0.10211</td>\n",
       "      <td>-0.326056</td>\n",
       "      <td>-0.054862</td>\n",
       "      <td>0.799561</td>\n",
       "      <td>-0.454147</td>\n",
       "      <td>0.651471</td>\n",
       "      <td>0.77149</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.209832</td>\n",
       "      <td>-0.007417</td>\n",
       "      <td>-0.781549</td>\n",
       "      <td>-0.601932</td>\n",
       "      <td>-0.389799</td>\n",
       "      <td>-0.287258</td>\n",
       "      <td>0.081594</td>\n",
       "      <td>0.251032</td>\n",
       "      <td>-0.294505</td>\n",
       "      <td>-1.212092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250325</th>\n",
       "      <td>-0.613347</td>\n",
       "      <td>0.05193</td>\n",
       "      <td>0.770452</td>\n",
       "      <td>0.10211</td>\n",
       "      <td>0.285841</td>\n",
       "      <td>-0.868831</td>\n",
       "      <td>0.013041</td>\n",
       "      <td>-0.30434</td>\n",
       "      <td>0.336334</td>\n",
       "      <td>0.525055</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.203315</td>\n",
       "      <td>-0.399393</td>\n",
       "      <td>-0.343471</td>\n",
       "      <td>0.030382</td>\n",
       "      <td>-0.602427</td>\n",
       "      <td>-0.563076</td>\n",
       "      <td>0.09517</td>\n",
       "      <td>-0.264187</td>\n",
       "      <td>-1.386697</td>\n",
       "      <td>-0.981316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250326</th>\n",
       "      <td>-0.487712</td>\n",
       "      <td>-0.169218</td>\n",
       "      <td>2.538947</td>\n",
       "      <td>0.10211</td>\n",
       "      <td>-1.109047</td>\n",
       "      <td>-0.368238</td>\n",
       "      <td>0.520992</td>\n",
       "      <td>-0.373693</td>\n",
       "      <td>1.355345</td>\n",
       "      <td>0.209199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736458</td>\n",
       "      <td>-0.148753</td>\n",
       "      <td>-0.123647</td>\n",
       "      <td>-0.005081</td>\n",
       "      <td>0.063675</td>\n",
       "      <td>-0.191528</td>\n",
       "      <td>-0.076106</td>\n",
       "      <td>-0.203932</td>\n",
       "      <td>-0.642578</td>\n",
       "      <td>-1.854022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250327</th>\n",
       "      <td>-0.351071</td>\n",
       "      <td>-0.169469</td>\n",
       "      <td>0.886757</td>\n",
       "      <td>0.10211</td>\n",
       "      <td>0.138501</td>\n",
       "      <td>0.840338</td>\n",
       "      <td>0.169291</td>\n",
       "      <td>-0.214893</td>\n",
       "      <td>-0.272475</td>\n",
       "      <td>0.172172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273986</td>\n",
       "      <td>-0.82126</td>\n",
       "      <td>-0.314554</td>\n",
       "      <td>-0.481238</td>\n",
       "      <td>0.078668</td>\n",
       "      <td>-0.162028</td>\n",
       "      <td>-0.165035</td>\n",
       "      <td>-0.53888</td>\n",
       "      <td>-0.290816</td>\n",
       "      <td>-0.418941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250328</th>\n",
       "      <td>-0.763016</td>\n",
       "      <td>-0.398741</td>\n",
       "      <td>0.443842</td>\n",
       "      <td>-0.025697</td>\n",
       "      <td>-2.760407</td>\n",
       "      <td>0.398558</td>\n",
       "      <td>1.019297</td>\n",
       "      <td>0.209368</td>\n",
       "      <td>0.96603</td>\n",
       "      <td>0.090042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.793039</td>\n",
       "      <td>-1.737131</td>\n",
       "      <td>-0.898847</td>\n",
       "      <td>-0.973034</td>\n",
       "      <td>0.010994</td>\n",
       "      <td>-0.794989</td>\n",
       "      <td>-0.618814</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>-0.189197</td>\n",
       "      <td>-0.543007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250331</th>\n",
       "      <td>-0.598224</td>\n",
       "      <td>0.553327</td>\n",
       "      <td>-0.025697</td>\n",
       "      <td>-0.025697</td>\n",
       "      <td>1.077639</td>\n",
       "      <td>-0.210389</td>\n",
       "      <td>0.63458</td>\n",
       "      <td>0.47095</td>\n",
       "      <td>0.96426</td>\n",
       "      <td>1.14686</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256336</td>\n",
       "      <td>0.07911</td>\n",
       "      <td>-0.565669</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>0.566957</td>\n",
       "      <td>0.174849</td>\n",
       "      <td>0.473107</td>\n",
       "      <td>0.081662</td>\n",
       "      <td>0.725858</td>\n",
       "      <td>-0.245556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57 rows × 5269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            000001    000002    000004    000005    000006    000007  \\\n",
       "20250102 -0.904052  0.089991  2.092479  0.134961 -0.049566  0.228958   \n",
       "20250103 -0.489799 -0.567948   0.84887  0.134961  -0.21165 -1.765285   \n",
       "20250106 -0.338798  0.357211     3.631  0.134961  0.347434  0.238773   \n",
       "20250107 -0.760277 -0.044117  1.462992  0.134961 -0.204369 -0.433124   \n",
       "20250108  -0.44256  0.029288  2.236714  0.134961  0.399007 -0.743933   \n",
       "20250109 -0.490571  0.188457  1.200161  0.188636  -0.18551 -0.293889   \n",
       "20250110 -0.365009 -0.288838  0.497971  0.188636 -0.349784 -1.189376   \n",
       "20250113  0.213688   0.71962  1.830424  0.188636  0.894913  1.023854   \n",
       "20250114 -1.101374 -0.035389 -0.252416  0.188636 -0.312758 -0.402604   \n",
       "20250115 -0.144947 -0.122055  0.161258  0.188636 -0.272139 -0.367372   \n",
       "20250116  -0.83374 -0.205608 -0.014233  0.481172 -0.511956 -1.219377   \n",
       "20250117   0.71659  1.378049  1.481304  0.481172  0.110378  0.103297   \n",
       "20250120 -0.636079  0.979383 -0.426423  0.481172 -0.736014 -0.245956   \n",
       "20250121 -0.437636  0.496434  0.481172  0.481172  0.289929 -0.445273   \n",
       "20250122  0.413085  2.319715  0.481172  0.481172  2.788935  0.748651   \n",
       "20250123 -1.187324 -0.600821  0.509987  0.509987 -0.380372 -0.421434   \n",
       "20250124  0.354776  1.345425  0.509987  0.509987  1.513576  0.291408   \n",
       "20250127   0.38319  0.060693  0.509987  0.509987  0.365421  0.140999   \n",
       "20250205 -0.665536  0.610094  0.509987  0.509987  0.339387 -0.617755   \n",
       "20250206  0.194748  1.356969  0.509987  0.509987  0.494476  0.426212   \n",
       "20250207 -0.462197  0.618074  0.479445  0.479445  0.993662 -0.210399   \n",
       "20250210 -0.228507  1.436073  0.479445  0.479445  1.743286  0.241101   \n",
       "20250211  0.171297  1.038285  0.479445  0.479445   0.81585  0.657488   \n",
       "20250212  0.312925  0.834172  0.479445  0.479445  1.130847  1.022984   \n",
       "20250213 -0.277921  0.786492  0.479445  0.479445  0.819894  0.631549   \n",
       "20250214 -0.082182  1.289042  0.260918  0.260918  0.713305   0.04205   \n",
       "20250217 -0.211188  0.862637  0.260918  0.260918  0.128421 -0.092509   \n",
       "20250218  0.177768  1.109487  0.260918  0.260918  0.641296  1.116441   \n",
       "20250219  0.497883  0.601546  0.260918  0.260918  0.795135  0.969581   \n",
       "20250220   0.11167 -0.067062  0.260918  0.260918  0.638411   0.30996   \n",
       "20250221  0.212058  0.480046  0.253018  0.253018  -1.15476  0.569544   \n",
       "20250224  0.092816  0.765087  0.253018  0.253018  1.112577  0.963884   \n",
       "20250225  0.356005  1.250742  0.253018  0.253018  1.597188  1.017963   \n",
       "20250226  0.313859  0.187905  0.253018  0.253018  0.670144  0.272139   \n",
       "20250227 -0.352533  0.247165  0.253018  0.253018  0.425912  0.847922   \n",
       "20250228  0.612892  0.787548  0.307781  0.307781  2.919862  0.864507   \n",
       "20250303 -0.190661  1.334926  0.307781  0.307781  0.104649  0.395411   \n",
       "20250304  0.449972  0.927822  0.307781  0.307781  1.180857  0.591738   \n",
       "20250305 -0.256351  0.237364  0.307781  0.307781 -0.262909  0.156843   \n",
       "20250306 -0.130246 -0.458354  0.307781  0.307781  -0.81974 -0.434596   \n",
       "20250307   0.59883  0.043957  0.273457  0.273457 -0.040834  0.518876   \n",
       "20250310   0.45205  0.104582  0.273457  0.273457 -0.707551  0.022267   \n",
       "20250311  0.639491  0.762582  0.273457  0.273457 -2.519167  0.755088   \n",
       "20250312 -0.808633 -0.483864  0.273457  0.273457  0.251129  0.016837   \n",
       "20250313  0.181348  0.457114  0.273457  0.273457  0.062277 -0.389263   \n",
       "20250314  0.758573   0.03519  0.364875  0.364875 -0.019026  0.199495   \n",
       "20250317 -0.689106 -0.133325  0.364875  0.364875  0.062284  0.322409   \n",
       "20250318  -0.25353  0.276067  0.364875  0.364875 -0.259088 -0.885714   \n",
       "20250319 -0.236468   0.26552  0.264722  0.364875  0.422298  0.304992   \n",
       "20250320 -0.242419  0.221612  0.901015  0.364875  0.253279  0.455646   \n",
       "20250321    -0.237  0.528542  1.517993   0.10211  0.219791 -0.167149   \n",
       "20250324 -0.231402 -0.403474  0.279134   0.10211 -0.326056 -0.054862   \n",
       "20250325 -0.613347   0.05193  0.770452   0.10211  0.285841 -0.868831   \n",
       "20250326 -0.487712 -0.169218  2.538947   0.10211 -1.109047 -0.368238   \n",
       "20250327 -0.351071 -0.169469  0.886757   0.10211  0.138501  0.840338   \n",
       "20250328 -0.763016 -0.398741  0.443842 -0.025697 -2.760407  0.398558   \n",
       "20250331 -0.598224  0.553327 -0.025697 -0.025697  1.077639 -0.210389   \n",
       "\n",
       "            000008    000009    000010    000011  ...    688787    688788  \\\n",
       "20250102  0.056152 -0.138607  2.041149  0.288532  ...  0.081127 -0.676493   \n",
       "20250103 -0.096718   -0.0477  0.573878  1.203264  ... -0.781348 -0.170623   \n",
       "20250106  2.492587  0.481124   0.95551  1.157385  ... -0.458895  0.621874   \n",
       "20250107  0.411026  0.013601  0.214352  0.883486  ... -0.096836 -0.432892   \n",
       "20250108  1.071878 -0.258853  0.665936  0.528503  ... -0.976907 -0.773776   \n",
       "20250109  0.660982  0.789255  0.621765  0.152752  ...  0.123769  0.083743   \n",
       "20250110   0.65501  0.214464  0.092414 -0.083319  ... -4.702905 -0.652699   \n",
       "20250113    0.8168  1.040991  0.569226  0.794444  ...  1.225399  0.456504   \n",
       "20250114  0.313011 -0.418676 -1.044771 -0.479961  ... -1.416077 -0.278662   \n",
       "20250115 -0.054225 -0.145526 -0.761952 -0.006172  ... -1.662644  0.398402   \n",
       "20250116  0.099685 -0.197575 -1.258955  -0.55858  ... -1.506233 -1.221387   \n",
       "20250117  2.666393  0.586274  1.210542  0.979752  ... -0.485306  0.040027   \n",
       "20250120  0.139384 -0.263891 -0.398975  0.194464  ... -3.154701   0.03157   \n",
       "20250121  1.301967 -0.467205  0.436877  0.114003  ...  -4.02098 -0.138552   \n",
       "20250122  2.171725  0.536698   1.01853  0.096151  ... -0.047342 -0.230831   \n",
       "20250123  0.122731 -0.205242 -0.294128 -0.054435  ...  -1.55001 -0.521434   \n",
       "20250124  0.633418  0.408217   1.14671  1.553364  ... -0.268643  0.168638   \n",
       "20250127  0.924113  0.587986   0.22245  0.662811  ... -6.231331  0.059523   \n",
       "20250205 -0.324231 -0.020597 -1.340413  0.326736  ...  -3.78613  0.153687   \n",
       "20250206  1.621534  0.528174  0.350154  0.727002  ... -1.709728 -0.061796   \n",
       "20250207 -0.241515  0.145889  1.327986   0.68834  ...  -2.04485 -0.011086   \n",
       "20250210 -0.076097 -0.267054  0.487968   0.44234  ... -1.313093 -0.009591   \n",
       "20250211  0.050522 -0.114854  0.268683   0.23661  ... -1.967903  0.237451   \n",
       "20250212  0.023015 -0.015548  0.844926  0.875878  ... -1.262587  -0.06065   \n",
       "20250213  0.919848  0.560483  0.842274   1.17639  ... -0.578134  0.394233   \n",
       "20250214  0.774715 -0.229435 -0.722213  0.118425  ... -0.633342  0.559451   \n",
       "20250217 -0.133109 -0.112063 -0.041085  0.542998  ... -1.033899 -0.179534   \n",
       "20250218  0.094352  0.336046  0.065078  0.431888  ... -1.807196 -2.199323   \n",
       "20250219   0.99955  0.328108  1.114255  1.383636  ... -0.849816  0.328449   \n",
       "20250220   -0.3195  0.227155  0.172854   0.43558  ... -0.430836 -0.872055   \n",
       "20250221  0.946206  0.050251  0.493398  0.374904  ... -1.878866 -1.687277   \n",
       "20250224 -0.436032  0.136437  0.215957  0.808383  ... -1.486165 -0.401103   \n",
       "20250225  0.712668  0.688047  1.893106  1.509907  ... -0.206328 -0.638099   \n",
       "20250226 -0.102237  0.365359  -0.64588  0.699616  ... -1.046828 -0.612404   \n",
       "20250227  0.057561  0.256181   0.33191  0.529335  ... -0.897373 -0.330388   \n",
       "20250228  1.781937  0.752383  0.424391  1.034583  ...  0.334181  0.082271   \n",
       "20250303  1.600276 -0.328663  0.052872  0.375762  ... -0.118165 -1.234503   \n",
       "20250304  0.463992  0.461512  0.831143  0.685321  ...  0.350758  -0.01777   \n",
       "20250305 -0.625579 -0.112959 -0.508872 -0.378252  ...  -0.10093 -0.795031   \n",
       "20250306  0.356716 -0.864816  0.181316 -0.196416  ... -0.811404 -1.065266   \n",
       "20250307  0.393081  0.240779 -0.063037   0.13689  ... -0.285011  -0.33447   \n",
       "20250310 -0.901914  0.153245  0.296254  0.152991  ... -0.251678 -2.060071   \n",
       "20250311  0.411208  0.860003  0.746597  0.875219  ...   0.12352  0.203004   \n",
       "20250312 -0.258457 -0.415855 -0.310729  0.244227  ... -1.439826 -2.527123   \n",
       "20250313  0.516293  0.338254   0.83128  0.329332  ... -0.881482 -0.332239   \n",
       "20250314  0.632333  0.198539  0.593357   0.37929  ... -1.250091 -1.127409   \n",
       "20250317  1.009955  0.309609  1.018921  0.278765  ... -1.380948 -1.999367   \n",
       "20250318 -0.501194 -0.441942  0.907131  0.027689  ... -0.895608 -3.214111   \n",
       "20250319  0.643591  0.493778  0.975399  0.827988  ...  0.431055 -1.909187   \n",
       "20250320  0.590378  0.600963  1.313962  0.876602  ... -0.745062 -0.564776   \n",
       "20250321 -0.150813  0.188086  0.298946  0.489481  ... -1.208437 -0.632284   \n",
       "20250324  0.799561 -0.454147  0.651471   0.77149  ... -1.209832 -0.007417   \n",
       "20250325  0.013041  -0.30434  0.336334  0.525055  ... -0.203315 -0.399393   \n",
       "20250326  0.520992 -0.373693  1.355345  0.209199  ...  0.736458 -0.148753   \n",
       "20250327  0.169291 -0.214893 -0.272475  0.172172  ...  0.273986  -0.82126   \n",
       "20250328  1.019297  0.209368   0.96603  0.090042  ... -0.793039 -1.737131   \n",
       "20250331   0.63458   0.47095   0.96426   1.14686  ...  0.256336   0.07911   \n",
       "\n",
       "            688789    688793    688798    688799    688800    688819  \\\n",
       "20250102 -0.664498  0.814576  0.244681 -0.675281 -0.749205 -0.114281   \n",
       "20250103 -0.691842  0.263831 -0.373882 -0.664265 -1.056103 -0.211608   \n",
       "20250106 -0.534907  0.407573  0.126543 -0.479251 -0.397901 -0.459894   \n",
       "20250107 -0.856083 -1.303184  0.171071  -0.64426 -1.094856 -0.236792   \n",
       "20250108 -0.696915 -0.606397 -1.063534 -0.268603 -0.358294 -0.681916   \n",
       "20250109  0.230231 -0.090631 -1.008725  0.108681 -3.517992 -0.529102   \n",
       "20250110 -0.626355   0.04872 -0.291953 -0.627871 -2.061929 -0.068576   \n",
       "20250113 -0.436244  1.011631  -0.20339 -0.037062 -0.226455   0.40153   \n",
       "20250114  -0.62209  0.330344 -0.086687 -0.571023 -2.081038 -1.108473   \n",
       "20250115 -1.194339 -0.457727 -0.851418 -0.517234 -2.329559 -0.902747   \n",
       "20250116 -1.148745  0.352039 -0.413072 -0.513386 -0.475044 -0.473702   \n",
       "20250117  -0.23091   0.22723  0.173033 -0.165955   0.27819  0.062366   \n",
       "20250120 -1.281569 -0.621922 -0.699626 -0.392538 -0.841374 -0.568553   \n",
       "20250121 -0.644838 -0.641193 -0.286198 -0.237758  0.169772  -1.22119   \n",
       "20250122 -0.136118  0.962964  0.028411  0.029583  0.294853  0.216354   \n",
       "20250123 -1.101248  0.517351 -0.519873  -0.72536 -1.133254 -0.702542   \n",
       "20250124 -0.487115  0.648739 -0.840442 -0.252846  1.047189  0.231064   \n",
       "20250127 -0.682082  0.176004  0.035276 -0.657422 -0.408648 -0.599464   \n",
       "20250205  -1.04286  0.093426 -0.427195 -0.439719 -0.081822  -0.51589   \n",
       "20250206  0.948837  0.533935  0.043839  -0.24486 -0.067107  0.212994   \n",
       "20250207 -0.295617  0.157252 -0.851897 -0.734983 -0.376432  -0.36213   \n",
       "20250210  0.051791  0.370303 -0.540463  0.050606  -0.28923  -0.14355   \n",
       "20250211 -0.563694 -1.343087 -0.205119 -0.265016 -0.232323  0.076955   \n",
       "20250212 -0.267067 -0.207975  0.578494 -0.190099 -1.019712  0.272052   \n",
       "20250213 -1.138999  0.188936 -0.318727  0.186141   -1.1158 -0.311047   \n",
       "20250214 -0.096406  0.072936 -0.152379  0.471758  0.030303 -0.255451   \n",
       "20250217 -0.732763  0.539017 -0.309814 -0.279501 -0.183784 -0.176494   \n",
       "20250218  0.025144   0.17397 -0.213371  0.180576   0.09672 -0.627554   \n",
       "20250219 -0.586084 -0.187158 -0.067611 -0.083202  0.080432 -0.840097   \n",
       "20250220 -0.199961 -0.149572 -0.270188 -0.083423 -0.042412   0.06029   \n",
       "20250221 -0.508964 -0.029213 -0.702672 -0.115967  -0.00501 -0.301042   \n",
       "20250224 -0.004293  0.032972   0.07202  0.187267   0.27004 -0.823024   \n",
       "20250225  0.411152  0.708137  0.299435  0.035955  0.680142  0.238159   \n",
       "20250226 -0.748629  0.173367 -0.310093 -0.401008 -0.539452  0.369914   \n",
       "20250227 -0.205192  0.234017 -2.344728 -0.272062 -1.716527 -0.177447   \n",
       "20250228  0.178394  1.056943 -0.316552  0.280214 -0.109341   -0.0798   \n",
       "20250303  -1.28424 -0.070713  0.190068 -0.707869 -0.740117  0.391974   \n",
       "20250304 -0.569366  0.532062  0.270578 -0.135028  0.630126  0.749328   \n",
       "20250305 -1.261865  0.366697 -1.559003  -0.33626 -0.479428 -0.761519   \n",
       "20250306 -0.456791 -1.516444 -0.836757 -0.576073 -0.520103 -0.299478   \n",
       "20250307  -0.73926 -0.034199 -0.692943  0.068347 -0.183223   0.08561   \n",
       "20250310 -0.536677 -0.859122  0.122379 -0.568772 -0.437408 -0.426997   \n",
       "20250311  0.759749  0.379899  0.278806  0.031479  1.076025  0.811853   \n",
       "20250312 -0.692771 -0.889919 -0.553083 -0.315473 -0.626193 -0.412816   \n",
       "20250313 -0.075963   0.81813 -0.150763  -0.18858 -0.647618   0.26041   \n",
       "20250314 -0.398753 -0.289377 -0.301693   0.20669 -1.747086 -0.013087   \n",
       "20250317 -0.443132  0.223071  0.276311 -0.955541 -0.949314  0.163168   \n",
       "20250318 -0.131925 -0.107304 -0.759922 -0.023932 -0.731775  -0.45467   \n",
       "20250319  0.394311  0.104988   0.17699  0.189081 -0.239644   0.60685   \n",
       "20250320 -0.515208  -0.15012  0.237059  0.011149 -0.074946  0.383264   \n",
       "20250321 -0.103244  0.510846 -0.566127 -0.499798  0.001045 -0.120563   \n",
       "20250324 -0.781549 -0.601932 -0.389799 -0.287258  0.081594  0.251032   \n",
       "20250325 -0.343471  0.030382 -0.602427 -0.563076   0.09517 -0.264187   \n",
       "20250326 -0.123647 -0.005081  0.063675 -0.191528 -0.076106 -0.203932   \n",
       "20250327 -0.314554 -0.481238  0.078668 -0.162028 -0.165035  -0.53888   \n",
       "20250328 -0.898847 -0.973034  0.010994 -0.794989 -0.618814 -0.000904   \n",
       "20250331 -0.565669  0.014365  0.566957  0.174849  0.473107  0.081662   \n",
       "\n",
       "            688981    689009  \n",
       "20250102  0.767347 -0.409309  \n",
       "20250103 -0.904616 -1.180849  \n",
       "20250106 -0.080859 -0.578569  \n",
       "20250107 -0.617414 -1.019745  \n",
       "20250108 -2.059001  -1.08844  \n",
       "20250109 -0.190433 -0.634504  \n",
       "20250110 -0.497818 -1.076716  \n",
       "20250113  0.463044  0.238483  \n",
       "20250114 -2.719101 -0.428268  \n",
       "20250115 -1.388252 -0.483791  \n",
       "20250116 -0.972741 -0.151894  \n",
       "20250117  1.491873  0.396571  \n",
       "20250120 -1.620166 -0.411724  \n",
       "20250121 -1.463238 -0.702478  \n",
       "20250122  0.484573  0.322657  \n",
       "20250123 -1.177685 -1.782798  \n",
       "20250124  0.945011  0.323267  \n",
       "20250127 -0.724188 -3.532338  \n",
       "20250205 -1.562917 -1.534113  \n",
       "20250206  1.416902  0.138353  \n",
       "20250207 -1.040764 -0.084758  \n",
       "20250210 -1.042885  -0.01569  \n",
       "20250211 -1.104271  0.686453  \n",
       "20250212  0.113853 -0.285097  \n",
       "20250213 -1.272015  0.227165  \n",
       "20250214   0.47977  0.049429  \n",
       "20250217 -1.348357 -1.520508  \n",
       "20250218 -1.263454 -0.030244  \n",
       "20250219 -0.532761 -0.511964  \n",
       "20250220 -0.469854 -1.351192  \n",
       "20250221 -0.575859 -1.052428  \n",
       "20250224  0.034948 -0.367174  \n",
       "20250225 -0.280613  0.936119  \n",
       "20250226 -2.444743 -0.708457  \n",
       "20250227 -0.467317 -0.889173  \n",
       "20250228  0.024783  0.029961  \n",
       "20250303  -0.27074 -0.139644  \n",
       "20250304  0.075274  0.015627  \n",
       "20250305 -0.969183 -0.556107  \n",
       "20250306 -1.887144 -1.445656  \n",
       "20250307  -0.45157  0.092991  \n",
       "20250310  0.269505 -1.050106  \n",
       "20250311  0.130277  0.940331  \n",
       "20250312 -0.919056 -0.314424  \n",
       "20250313  0.217892 -0.626144  \n",
       "20250314  0.163502 -0.395579  \n",
       "20250317 -0.857937 -2.264636  \n",
       "20250318 -0.972818 -0.317917  \n",
       "20250319  0.049232  1.016428  \n",
       "20250320 -0.384802  -0.48722  \n",
       "20250321   0.01431 -1.272805  \n",
       "20250324 -0.294505 -1.212092  \n",
       "20250325 -1.386697 -0.981316  \n",
       "20250326 -0.642578 -1.854022  \n",
       "20250327 -0.290816 -0.418941  \n",
       "20250328 -0.189197 -0.543007  \n",
       "20250331  0.725858 -0.245556  \n",
       "\n",
       "[57 rows x 5269 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_factor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
